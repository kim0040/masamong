# -*- coding: utf-8 -*-
import discord
from discord.ext import commands
import google.generativeai as genai
from datetime import datetime, timedelta, time
import asyncio
import pytz
from collections import deque
from typing import Dict, Any, Tuple
import sqlite3
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle

import config
from logger_config import logger
import utils

KST = pytz.timezone('Asia/Seoul')

def _cosine_similarity(v1, v2):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

class AIHandler(commands.Cog):
    """Gemini AI ìƒí˜¸ì‘ìš© (ìë°œì  ì‘ë‹µ, ì˜ë„ ë¶„ì„, ì‚¬ìš©ìë³„ ëŒ€í™” ê¸°ë¡)"""

    def __init__(self, bot: commands.Bot):
        self.bot = bot
        self.ai_user_cooldowns: Dict[int, datetime] = {}
        self.gemini_configured = False
        self.api_call_lock = asyncio.Lock()
        self.minute_request_timestamps = deque()
        self.last_proactive_response_times: Dict[int, datetime] = {}
        self.embedding_model = None

        if config.GEMINI_API_KEY:
            try:
                genai.configure(api_key=config.GEMINI_API_KEY)
                self.model = genai.GenerativeModel(config.AI_MODEL_NAME)
                self.intent_model = genai.GenerativeModel(config.AI_INTENT_MODEL_NAME)
                logger.info("Gemini API ë° ëª¨ë¸ ì„¤ì • ì™„ë£Œ.")
                self.gemini_configured = True
            except Exception as e:
                logger.critical(f"Gemini API ì„¤ì • ì‹¤íŒ¨: {e}. AI ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨.", exc_info=True)

        try:
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("SentenceTransformer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            logger.critical(f"SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨.", exc_info=True)

    @property
    def is_ready(self) -> bool:
        """AI í•¸ë“¤ëŸ¬ê°€ ëª¨ë“  ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return self.gemini_configured and self.embedding_model is not None

    async def _check_global_rate_limit(self) -> Tuple[bool, str | None]:
        async with self.api_call_lock:
            now = datetime.now()
            one_minute_ago = now - timedelta(minutes=1)
            while self.minute_request_timestamps and self.minute_request_timestamps[0] < one_minute_ago:
                self.minute_request_timestamps.popleft()

            if len(self.minute_request_timestamps) >= config.API_RPM_LIMIT:
                logger.warning(f"ë¶„ë‹¹ Gemini API í˜¸ì¶œ ì œí•œ ë„ë‹¬ ({len(self.minute_request_timestamps)}/{config.API_RPM_LIMIT}).")
                return True, config.MSG_AI_RATE_LIMITED

        if await utils.is_api_limit_reached('gemini_daily_calls', config.API_RPD_LIMIT):
            return True, config.MSG_AI_DAILY_LIMITED

        return False, None

    def _record_api_call(self):
        """API í˜¸ì¶œì„ ê¸°ë¡í•©ë‹ˆë‹¤ (ë¶„ë‹¹ ì œí•œìš©)."""
        now = datetime.now()
        self.minute_request_timestamps.append(now)
        logger.debug(f"Gemini API í˜¸ì¶œ ê¸°ë¡ë¨. (ì§€ë‚œ 1ë¶„ê°„: {len(self.minute_request_timestamps)}íšŒ)")

    def _is_on_cooldown(self, user_id: int) -> Tuple[bool, float]:
        now = datetime.now()
        if user_id in self.ai_user_cooldowns:
            time_since_last = now - self.ai_user_cooldowns[user_id]
            if time_since_last.total_seconds() < config.AI_COOLDOWN_SECONDS:
                return True, config.AI_COOLDOWN_SECONDS - time_since_last.total_seconds()
        return False, 0.0

    def _update_cooldown(self, user_id: int):
        self.ai_user_cooldowns[user_id] = datetime.now()
        cutoff_time = datetime.now() - timedelta(seconds=config.AI_COOLDOWN_SECONDS * 10)
        self.ai_user_cooldowns = {uid: t for uid, t in self.ai_user_cooldowns.items() if t >= cutoff_time}

    def is_proactive_on_cooldown(self, channel_id: int) -> bool:
        cooldown_seconds = config.AI_PROACTIVE_RESPONSE_CONFIG.get("cooldown_seconds", 90)
        last_time = self.last_proactive_response_times.get(channel_id)
        if last_time and (datetime.now() - last_time).total_seconds() < cooldown_seconds:
            logger.debug(f"ì±„ë„({channel_id}) ìë°œì  ì‘ë‹µ ì¿¨ë‹¤ìš´ ì¤‘.")
            return True
        return False

    def update_proactive_cooldown(self, channel_id: int):
        self.last_proactive_response_times[channel_id] = datetime.now()
        logger.info(f"ì±„ë„({channel_id}) ìë°œì  ì‘ë‹µ ì¿¨ë‹¤ìš´ ì‹œì‘.")

    def add_message_to_history(self, message: discord.Message):
        """ëŒ€í™” ê¸°ë¡ì„ DBì— ì €ì¥í•˜ê³ , ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not config.AI_MEMORY_ENABLED or not message.guild:
            return

        channel_config = config.CHANNEL_AI_CONFIG.get(message.channel.id, {})
        if not channel_config.get("allowed", False):
            return

        conn = None
        try:
            conn = sqlite3.connect(f"file:{config.DATABASE_FILE}?mode=rw", uri=True)
            cursor = conn.cursor()

            cursor.execute("""
                INSERT INTO conversation_history (message_id, guild_id, channel_id, user_id, user_name, content, is_bot, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                message.id,
                message.guild.id,
                message.channel.id,
                message.author.id,
                message.author.display_name,
                message.content,
                message.author.bot,
                message.created_at.isoformat()
            ))
            conn.commit()
            logger.debug(f"[{message.guild.name}/{message.channel.name}] ë©”ì‹œì§€ ID {message.id}ë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

            if not message.author.bot and self.embedding_model:
                asyncio.create_task(self._create_and_save_embedding(message.id, message.content))

        except sqlite3.Error as e:
            logger.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ DB ì˜¤ë¥˜: {e}", exc_info=True)
        finally:
            if conn:
                conn.close()

    async def _create_and_save_embedding(self, message_id: int, content: str):
        """ì£¼ì–´ì§„ ë‚´ìš©ì˜ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.embedding_model: return
        
        try:
            logger.debug(f"ë©”ì‹œì§€ ID {message_id}ì˜ ì„ë² ë”© ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            embedding = await asyncio.to_thread(self.embedding_model.encode, content)
            embedding_blob = pickle.dumps(embedding)

            conn = None
            try:
                conn = sqlite3.connect(f"file:{config.DATABASE_FILE}?mode=rw", uri=True)
                cursor = conn.cursor()
                cursor.execute("UPDATE conversation_history SET embedding = ? WHERE message_id = ?", (embedding_blob, message_id))
                conn.commit()
                logger.info(f"ë©”ì‹œì§€ ID {message_id}ì˜ ì„ë² ë”©ì„ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except sqlite3.Error as e:
                logger.error(f"ì„ë² ë”© ì €ì¥ ì¤‘ DB ì˜¤ë¥˜: {e}", exc_info=True)
            finally:
                if conn:
                    conn.close()

        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„±/ì €ì¥ ì¤‘ ì˜¤ë¥˜ (ë©”ì‹œì§€ ID: {message_id}): {e}", exc_info=True)

    async def _find_similar_conversations(self, channel_id: int, user_id: int, query_embedding: np.ndarray, top_k: int = 5) -> str:
        """DBì—ì„œ ìœ ì‚¬í•œ ëŒ€í™”ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        conn = None
        try:
            conn = sqlite3.connect(f"file:{config.DATABASE_FILE}?mode=ro", uri=True)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT content, embedding FROM conversation_history
                WHERE channel_id = ? AND user_id = ? AND embedding IS NOT NULL
                ORDER BY created_at DESC
                LIMIT 100;
            """, (channel_id, user_id))
            
            rows = cursor.fetchall()
            if not rows: return ""

            similarities = []
            for content, embedding_blob in rows:
                embedding = pickle.loads(embedding_blob)
                sim = _cosine_similarity(query_embedding, embedding)
                similarities.append((sim, content))
            
            similarities.sort(key=lambda x: x[0], reverse=True)
            top_conversations = [content for sim, content in similarities[:top_k]]

            if not top_conversations: return ""

            context_str = "ì´ì „ ëŒ€í™” ì¤‘ ê´€ë ¨ ë‚´ìš©:\n" + "\n".join(f"- {conv}" for conv in reversed(top_conversations))
            return context_str

        except sqlite3.Error as e:
            logger.error(f"ìœ ì‚¬ ëŒ€í™” ê²€ìƒ‰ ì¤‘ DB ì˜¤ë¥˜: {e}", exc_info=True)
            return ""
        finally:
            if conn:
                conn.close()

    async def should_proactively_respond(self, message: discord.Message) -> bool:
        return False

    async def analyze_intent(self, message: discord.Message) -> str:
        if not config.AI_INTENT_ANALYSIS_ENABLED or not self.is_ready: return "Chat"
        
        user_query = message.content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>', '').strip()
        if not user_query: return "Chat"
        
        try:
            is_limited, _ = await self._check_global_rate_limit()
            if is_limited: return "Chat"
            
            prompt = f"{config.AI_INTENT_PERSONA}\n\nì‚¬ìš©ì ë©”ì‹œì§€: \"{user_query}\""
            logger.debug(f"ì˜ë„ ë¶„ì„ ìš”ì²­: {user_query[:50]}...")
            
            async with self.api_call_lock:
                self._record_api_call()
                response = await self.intent_model.generate_content_async(prompt)
                await utils.increment_api_counter('gemini_daily_calls')
            
            intent = response.text.strip()
            logger.info(f"ì˜ë„ ë¶„ì„ ê²°ê³¼: '{intent}' (ì›ë³¸: '{user_query[:50]}...')")
            
            valid_intents = ['Time', 'Weather', 'Command', 'Chat', 'Mixed']
            return intent if intent in valid_intents else 'Chat'
        except Exception as e:
            logger.error(f"AI ì˜ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return "Chat"

    async def generate_creative_text(self, channel: discord.TextChannel, author: discord.User, prompt_key: str, context: Dict[str, Any] | None = None) -> str | None:
        if not self.is_ready:
            logger.warning(f"ì°½ì˜ì  í…ìŠ¤íŠ¸ ìƒì„± ë¶ˆê°€({prompt_key}): AI í•¸ë“¤ëŸ¬ ë¯¸ì¤€ë¹„.")
            return config.MSG_AI_ERROR

        prompt_template = config.AI_CREATIVE_PROMPTS.get(prompt_key)
        if not prompt_template:
            logger.error(f"ì°½ì˜ì  í…ìŠ¤íŠ¸ ìƒì„± ë¶ˆê°€: configì—ì„œ í”„ë¡¬í”„íŠ¸ í‚¤ '{prompt_key}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ.")
            return config.MSG_CMD_ERROR

        task_prompt = prompt_template.format(**(context or {}))
        channel_config = config.CHANNEL_AI_CONFIG.get(channel.id, {})

        return await self._generate_gemini_response(
            channel_id=channel.id,
            user_query=task_prompt,
            author=author,
            persona_config=channel_config,
            is_task=True
        )

    async def _generate_gemini_response(
        self,
        channel_id: int,
        user_query: str,
        author: discord.User,
        persona_config: dict,
        weather_info_str: str | None = None,
        is_task: bool = False
    ) -> str | None:
        if not self.is_ready: return config.MSG_AI_ERROR

        if not is_task:
            on_cooldown, remaining_time = self._is_on_cooldown(author.id)
            if on_cooldown:
                return config.MSG_AI_COOLDOWN.format(remaining=remaining_time)
            self._update_cooldown(author.id)

        is_limited, limit_message = await self._check_global_rate_limit()
        if is_limited: return limit_message

        user_persona_override = config.USER_SPECIFIC_PERSONAS.get(author.id)
        persona_cfg = user_persona_override or persona_config

        system_instructions = [
            persona_cfg.get("persona", ""),
            persona_cfg.get("rules", "")
        ]
        if weather_info_str:
            system_instructions.append(f"ì°¸ê³ í•  ë‚ ì”¨ ì •ë³´: {weather_info_str}")

        query_embedding = await asyncio.to_thread(self.embedding_model.encode, user_query)
        rag_context = await self._find_similar_conversations(channel_id, author.id, query_embedding)
        if rag_context:
            system_instructions.append(rag_context)

        history = []

        try:
            model = genai.GenerativeModel(
                config.AI_MODEL_NAME,
                safety_settings=config.GEMINI_SAFETY_SETTINGS,
                system_instruction="\n".join(filter(None, system_instructions))
            )
            
            chat_session = model.start_chat(history=history)
            final_query = user_query if is_task else f"User({author.id}|{author.display_name}): {user_query}"
            
            logger.debug(f"AI ì²˜ë¦¬ ì‹œì‘ | {final_query[:80]}...")

            async with self.api_call_lock:
                self._record_api_call()
                response = await chat_session.send_message_async(final_query, stream=False)
                await utils.increment_api_counter('gemini_daily_calls')

            ai_response_text = response.text.strip()
            logger.info(f"AI ì‘ë‹µ ìƒì„± ì„±ê³µ (ê¸¸ì´: {len(ai_response_text)}): {ai_response_text[:50]}...")
            return ai_response_text

        except (genai.types.BlockedPromptException, genai.types.StopCandidateException) as security_exception:
            logger.warning(f"AI ìš”ì²­/ì‘ë‹µ ì°¨ë‹¨ë¨ | ì˜¤ë¥˜: {security_exception}")
            if 'prompt' in str(security_exception).lower():
                return config.MSG_AI_BLOCKED_PROMPT
            else:
                return config.MSG_AI_BLOCKED_RESPONSE
        except Exception as e:
            logger.error(f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}", exc_info=True)
            return config.MSG_AI_ERROR

    async def process_ai_message(self, message: discord.Message, weather_info: str | None = None):
        if not self.is_ready: return

        channel_config = config.CHANNEL_AI_CONFIG.get(message.channel.id)
        if not channel_config or not channel_config.get("allowed", False):
            if weather_info:
                await message.reply(f"ğŸ“ {weather_info}", mention_author=False)
            return

        user_query = message.content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>', '').strip()
        if not user_query and not weather_info:
            await message.reply(config.MSG_AI_NO_CONTENT.format(bot_name=self.bot.user.name), mention_author=False)
            return

        async with message.channel.typing():
            ai_response_text = await self._generate_gemini_response(
                channel_id=message.channel.id,
                user_query=user_query,
                author=message.author,
                persona_config=channel_config,
                weather_info_str=weather_info
            )

            if ai_response_text:
                bot_response_message = await message.reply(ai_response_text[:2000], mention_author=False)
                self.add_message_to_history(bot_response_message)

    async def generate_system_alert_message(self, channel_id: int, alert_context_info: str, alert_type: str = "ì¼ë°˜ ì•Œë¦¼") -> str | None:
        if not self.is_ready:
            logger.warning(f"ì‹œìŠ¤í…œ ì•Œë¦¼({alert_type}) ìƒì„± ë¶ˆê°€: AI í•¸ë“¤ëŸ¬ ë¯¸ì¤€ë¹„.")
            return None

        channel_config = config.CHANNEL_AI_CONFIG.get(channel_id, {})
        logger.info(f"ì‹œìŠ¤í…œ {alert_type} ìƒì„± ìš”ì²­: ì±„ë„={channel_id}, ë‚´ìš©='{alert_context_info[:100]}...'")

        user_query_for_alert = f"ë‹¤ìŒ ìƒí™©ì„ ë„ˆì˜ í˜ë¥´ì†Œë‚˜ì— ë§ê²Œ ì±„ë„ì— ì•Œë ¤ì¤˜: '{alert_context_info}'"
        
        system_author = self.bot.user

        generated_text = await self._generate_gemini_response(
            channel_id=channel_id,
            user_query=user_query_for_alert,
            author=system_author,
            persona_config=channel_config,
            is_task=True
        )
        return generated_text

async def setup(bot: commands.Bot):
    await bot.add_cog(AIHandler(bot))
