# -*- coding: utf-8 -*-
"""
ë§ˆì‚¬ëª½ ë´‡ì˜ AI ìƒí˜¸ì‘ìš©ì„ ì´ê´„í•˜ëŠ” í•µì‹¬ Cogì…ë‹ˆë‹¤.

2-Step Agent ì•„í‚¤í…ì²˜ì— ë”°ë¼ ë‹¤ìŒì˜ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1.  **ì˜ë„ ë¶„ì„ (Lite Model)**: ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê°„ë‹¨í•œ ëŒ€í™”ì¸ì§€, ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.
2.  **ë„êµ¬ ì‹¤í–‰**: ë¶„ì„ëœ ê³„íšì— ë”°ë¼ `ToolsCog`ì˜ ë„êµ¬ë“¤ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
3.  **ë‹µë³€ ìƒì„± (Main Model)**: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
4.  **ëŒ€í™” ê¸°ë¡ ê´€ë¦¬**: RAG(Retrieval-Augmented Generation)ë¥¼ ìœ„í•´ ëŒ€í™” ë‚´ìš©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

from __future__ import annotations


import discord
from discord.ext import commands
try:
    import google.generativeai as genai
except ModuleNotFoundError:  # pragma: no cover - í™˜ê²½ì— ë”°ë¼ ì„¤ì¹˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
    genai = None

# CometAPIìš© OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸
try:
    from openai import AsyncOpenAI
except ModuleNotFoundError:  # pragma: no cover
    AsyncOpenAI = None

from datetime import datetime, timedelta, timezone
import asyncio
import pytz
from collections import deque
import re
from typing import Dict, Any, Tuple
import aiosqlite
# numpyëŠ” AI ë©”ëª¨ë¦¬ ê¸°ëŠ¥(RAG)ì—ì„œë§Œ í•„ìš”í•˜ë¯€ë¡œ, ì„¤ì¹˜ë˜ì§€ ì•Šì€ í™˜ê²½ì—ì„œë„ ì‹¤í–‰ë˜ë„ë¡ ê°€ë“œí•œë‹¤.
try:
    import numpy as np
except ModuleNotFoundError:  # pragma: no cover - ê²½ëŸ‰ ì„¤ì¹˜ í™˜ê²½ ê³ ë ¤
    np = None  # type: ignore
import random
import time
import json
import uuid
import requests

import config
from logger_config import logger
from utils import db as db_utils
from utils import http
from utils.embeddings import (
    DiscordEmbeddingStore,
    KakaoEmbeddingStore,
    get_embedding,
)
from database.bm25_index import BM25IndexManager
from utils.hybrid_search import HybridSearchEngine
from utils.hybrid_search import HybridSearchEngine
from utils.reranker import Reranker, RerankerConfig
from utils.api_handlers.finnhub import ALIAS_TO_TICKER  # [NEW] Import for robust stock detection

KST = pytz.timezone('Asia/Seoul')

class AIHandler(commands.Cog):
    """AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” Cogì…ë‹ˆë‹¤.

    - Lite/Flash Gemini ëª¨ë¸ì„ ì‚¬ìš©í•´ ì˜ë„ ë¶„ì„ê³¼ ì‘ë‹µ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - `ToolsCog`ì™€ í˜‘ë ¥í•´ ì™¸ë¶€ API í˜¸ì¶œ, í›„ì²˜ë¦¬, ì˜¤ë¥˜ ë³µêµ¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    - ëŒ€í™” ì €ì¥ì†Œ(RAG)ë¥¼ êµ¬ì¶•í•´ ì¥ê¸° ê¸°ì–µê³¼ ëŠ¥ë™í˜• ì œì•ˆì„ ì§€ì›í•©ë‹ˆë‹¤.
    """

    def __init__(self, bot: commands.Bot):
        self.bot = bot
        self.tools_cog = bot.get_cog('ToolsCog')
        self.ai_user_cooldowns: Dict[int, datetime] = {}
        self.proactive_cooldowns: Dict[int, float] = {}
        self.gemini_configured = False
        self.api_call_lock = asyncio.Lock()
        self.discord_embedding_store = DiscordEmbeddingStore(config.DISCORD_EMBEDDING_DB_PATH)
        self.kakao_embedding_store = KakaoEmbeddingStore(
            config.KAKAO_EMBEDDING_DB_PATH,
            config.KAKAO_EMBEDDING_SERVER_MAP,
        ) if config.KAKAO_EMBEDDING_DB_PATH or config.KAKAO_EMBEDDING_SERVER_MAP else None
        self.bm25_manager = BM25IndexManager(config.BM25_DATABASE_PATH) if config.BM25_DATABASE_PATH else None

        reranker: Reranker | None = None
        if config.RERANK_ENABLED and config.RAG_RERANKER_MODEL_NAME:
            reranker_config = RerankerConfig(
                model_name=config.RAG_RERANKER_MODEL_NAME,
                device=config.RAG_RERANKER_DEVICE,
                score_threshold=config.RAG_RERANKER_SCORE_THRESHOLD,
            )
            reranker = Reranker(reranker_config)
        self.reranker = reranker
        self.hybrid_search_engine = HybridSearchEngine(
            self.discord_embedding_store,
            self.kakao_embedding_store,
            self.bm25_manager,
            reranker=self.reranker,
        )
        self._window_buffers: dict[tuple[int, int], deque[dict[str, Any]]] = {}
        self._window_counts: dict[tuple[int, int], int] = {}
        self.debug_enabled = config.AI_DEBUG_ENABLED
        self._debug_log_len = getattr(config, "AI_DEBUG_LOG_MAX_LEN", 400)

        if config.GEMINI_API_KEY and genai:
            try:
                genai.configure(api_key=config.GEMINI_API_KEY)
                logger.info("Gemini APIê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                self.gemini_configured = True
            except Exception as e:
                logger.critical(f"Gemini API ì„¤ì • ì‹¤íŒ¨: {e}. AI ê´€ë ¨ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", exc_info=True)
        elif config.GEMINI_API_KEY and not genai:
            logger.critical("google-generativeai íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ Gemini ê¸°ëŠ¥ì„ ì´ˆê¸°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # CometAPI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Gemini ëŒ€ì²´)
        self.cometapi_client = None
        self.use_cometapi = config.USE_COMETAPI and config.COMETAPI_KEY
        if self.use_cometapi:
            if AsyncOpenAI:
                try:
                    self.cometapi_client = AsyncOpenAI(
                        base_url=config.COMETAPI_BASE_URL,
                        api_key=config.COMETAPI_KEY,
                    )
                    logger.info(f"CometAPI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸: {config.COMETAPI_MODEL}")
                except Exception as e:
                    logger.error(f"CometAPI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.use_cometapi = False
            else:
                logger.warning("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ CometAPIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.use_cometapi = False
        
        # [NEW] Location Cache from DB
        self.location_cache: set[str] = set()

    @property
    def is_ready(self) -> bool:
        """AI í•¸ë“¤ëŸ¬ê°€ ëª¨ë“  ì˜ì¡´ì„±(Gemini, DB, ToolsCog)ì„ í¬í•¨í•˜ì—¬ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return self.gemini_configured and self.bot.db is not None and self.tools_cog is not None

    def _debug(self, message: str, log_extra: dict[str, Any] | None = None) -> None:
        """ë””ë²„ê·¸ ì„¤ì •ì´ ì¼œì§„ ê²½ìš°ì—ë§Œ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
        if not self.debug_enabled:
            return
        if log_extra:
            logger.debug(message, extra=log_extra)
        else:
            logger.debug(message)

    def _truncate_for_debug(self, value: Any) -> str:
        """ê¸´ ë¬¸ìì—´ì„ ë¡œê·¸ìš©ìœ¼ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤."""
        if value is None:
            return ""
        rendered = str(value)
        max_len = self._debug_log_len
        if len(rendered) <= max_len:
            return rendered
        return rendered[:max_len] + "â€¦"

    def _format_prompt_debug(self, prompt: Any) -> str:
        """Gemini í”„ë¡¬í”„íŠ¸ë¥¼ JSON ë¬¸ìì—´ ë˜ëŠ” ì¼ë°˜ ë¬¸ìì—´ë¡œ ì¶•ì•½í•©ë‹ˆë‹¤."""
        try:
            if isinstance(prompt, (dict, list)):
                rendered = json.dumps(prompt, ensure_ascii=False)
            else:
                rendered = str(prompt)
        except Exception:
            rendered = repr(prompt)
        return self._truncate_for_debug(rendered)

    async def _load_location_cache(self):
        """DBì—ì„œ ì§€ì—­ëª… ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ ìºì‹±í•©ë‹ˆë‹¤."""
        if self.location_cache:
            return

        if not self.bot.db:
            return

        try:
            # 2ê¸€ì ì´ìƒì¸ ì§€ì—­ëª…ë§Œ ë¡œë“œ (1ê¸€ìëŠ” ì˜¤íƒì§€ ê°€ëŠ¥ì„± ë†’ìŒ)
            async with self.bot.db.execute("SELECT name FROM locations WHERE LENGTH(name) >= 2") as cursor:
                rows = await cursor.fetchall()
                if rows:
                    self.location_cache = {row['name'] for row in rows}
                    logger.info(f"DBì—ì„œ ì§€ì—­ëª… ë°ì´í„° {len(self.location_cache)}ê°œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ì§€ì—­ëª… ìºì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _message_has_valid_mention(self, message: discord.Message) -> bool:
        """ë©”ì‹œì§€ì— ë´‡ ë©˜ì…˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        bot_user = getattr(self.bot, "user", None)
        if bot_user is None:
            return False

        try:
            mentions = getattr(message, "mentions", []) or []
        except AttributeError:
            mentions = []
        if any(getattr(member, "id", None) == bot_user.id for member in mentions):
            return True

        # ì—­í•  ë©˜ì…˜ í™•ì¸
        found_role_ids = set()
        if message.content:
            found_role_ids = set(re.findall(r'<@&(\d+)>', message.content))
        
        guild = getattr(message, "guild", None)
        if found_role_ids and guild:
            guild_me = getattr(guild, "me", None)
            if guild_me:
                my_role_ids = {str(r.id) for r in guild_me.roles if r.id != guild.id}
                if not found_role_ids.isdisjoint(my_role_ids):
                    return True

        content = (message.content or "").lower()
        alias_candidates: set[str] = set()
        name = getattr(bot_user, "name", None)
        if name:
            alias_candidates.add(f"@{name.lower()}")
        display_name = getattr(bot_user, "display_name", None)
        if display_name:
            alias_candidates.add(f"@{display_name.lower()}")
        global_name = getattr(bot_user, "global_name", None)
        if global_name:
            alias_candidates.add(f"@{global_name.lower()}")

        guild = getattr(message, "guild", None)
        if guild is not None:
            guild_me = getattr(guild, "me", None)
            guild_display = getattr(guild_me, "display_name", None)
            if guild_display:
                alias_candidates.add(f"@{str(guild_display).lower()}")

        # ì‚¬ìš©ìë“¤ì´ ë‹¤ì–‘í•œ ë³„ì¹­ìœ¼ë¡œ ë¶€ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª¨ë“  ë³„ì¹­ì„ ì†Œë¬¸ìë¡œ ë¹„êµí•œë‹¤.
        alias_candidates = {alias for alias in alias_candidates if alias.strip("@")}
        return any(alias in content for alias in alias_candidates)

    def _strip_bot_references(self, content: str, guild: discord.Guild | None) -> str:
        """ë©”ì‹œì§€ ë‚´ìš©ì—ì„œ ë´‡ ë©˜ì…˜ ë° ë³„ì¹­ì„ ì œê±°í•©ë‹ˆë‹¤."""
        base_content = content or ""
        bot_user = getattr(self.bot, "user", None)
        if bot_user is None:
            return base_content.strip()

        patterns: set[str] = set()
        patterns.add(f"<@{bot_user.id}>")
        patterns.add(f"<@!{bot_user.id}>")

        # ì—­í•  ë©˜ì…˜ ì œê±° íŒ¨í„´ ì¶”ê°€
        if guild:
            guild_me = getattr(guild, "me", None)
            if guild_me:
                for role in guild_me.roles:
                    if role.id != guild.id:
                        patterns.add(f"<@&{role.id}>")

        for alias in (
            getattr(bot_user, "name", None),
            getattr(bot_user, "display_name", None),
            getattr(bot_user, "global_name", None),
        ):
            if alias:
                patterns.add(f"@{alias}")

        if guild is not None:
            guild_me = getattr(guild, "me", None)
            guild_display = getattr(guild_me, "display_name", None)
            if guild_display:
                patterns.add(f"@{guild_display}")

        patterns = {p for p in patterns if p}
        if not patterns:
            return base_content.strip()

        pattern = re.compile("|".join(re.escape(p) for p in patterns), flags=re.IGNORECASE)
        stripped = pattern.sub(" ", base_content)
        return re.sub(r"\s+", " ", stripped).strip()

    def _prepare_user_query(self, message: discord.Message, log_extra: dict[str, Any]) -> str | None:
        """ë©˜ì…˜ ê²€ì¦ í›„ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì •ì œí•©ë‹ˆë‹¤."""
        # [NEW] DMì—ì„œëŠ” ë©˜ì…˜ì´ ì—†ì–´ë„ ëŒ€í™” ê°€ëŠ¥ (ì—¬ê¸°ì„œ Noneì„ ë°˜í™˜í•˜ë©´ ëŒ€í™”ê°€ ì¢…ë£Œë˜ë¯€ë¡œ, DMì´ë©´ í†µê³¼ì‹œí‚´)
        if not message.guild:
            # DM: ë©˜ì…˜ ì œê±° (ìˆë‹¤ë©´)
            stripped = self._strip_bot_references(message.content or "", message.guild)
            if not stripped: # ë©˜ì…˜ë§Œ ìˆê³  ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°
                 self._debug("DM: ë©˜ì…˜ë§Œ ì¡´ì¬í•´ ì¿¼ë¦¬ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", log_extra)
                 return None
            self._debug(f"DM ì‚¬ìš©ì ì¿¼ë¦¬: {self._truncate_for_debug(stripped)}", log_extra)
            return stripped

        if not self._message_has_valid_mention(message):
            self._debug("ë©˜ì…˜ì´ ì—†ì–´ ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.", log_extra)
            logger.info("ë©˜ì…˜ì´ ì—†ëŠ” ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.", extra=log_extra)
            return None
        # ë©˜ì…˜ë§Œ í¬í•¨ëœ ë©”ì‹œì§€ëŠ” Gemini í˜¸ì¶œì„ ë§‰ê¸° ìœ„í•´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•œë‹¤.
        stripped = self._strip_bot_references(message.content or "", message.guild)
        if not stripped:
            self._debug("ë©˜ì…˜ë§Œ ì¡´ì¬í•´ ì¿¼ë¦¬ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", log_extra)
            logger.info("ë´‡ ë©˜ì…˜ë§Œ í¬í•¨ëœ ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.", extra=log_extra)
            return None
        self._debug(f"ì •ì œëœ ì‚¬ìš©ì ì¿¼ë¦¬: {self._truncate_for_debug(stripped)}", log_extra)
        return stripped

    async def _safe_generate_content(self, model: genai.GenerativeModel, prompt: Any, log_extra: dict, generation_config: genai.types.GenerationConfig = None) -> genai.types.GenerateContentResponse | None:
        """Gemini `generate_content_async` í˜¸ì¶œì„ ê°ì‹¸ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.

        Args:
            model (genai.GenerativeModel): ì‚¬ìš©í•  Gemini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤.
            prompt (Any): ëª¨ë¸ì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ë¯¸ë””ì–´ í˜ì´ë¡œë“œ.
            log_extra (dict): ë¡œê¹… ì‹œ ë¶€ê°€ ì •ë³´ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬.
            generation_config (GenerationConfig, optional): í•„ìš” ì‹œ ë®ì–´ì“¸ ìƒì„± ì„¤ì •.

        Returns:
            GenerateContentResponse | None: ì„±ê³µ ì‹œ Gemini ì‘ë‹µ, ì‹¤íŒ¨ ë˜ëŠ” ì†ë„ ì œí•œ ì‹œ None.
        """
        if generation_config is None:
            generation_config = genai.types.GenerationConfig(temperature=0.0)

        try:
            limit_key = 'gemini_intent' if config.AI_INTENT_MODEL_NAME in model.model_name else 'gemini_response'
            rpm = config.RPM_LIMIT_INTENT if limit_key == 'gemini_intent' else config.RPM_LIMIT_RESPONSE
            rpd = config.RPD_LIMIT_INTENT if limit_key == 'gemini_intent' else config.RPD_LIMIT_RESPONSE

            if self.debug_enabled:
                preview = self._format_prompt_debug(prompt)
                self._debug(f"[Gemini:{model.model_name}] í˜¸ì¶œ í”„ë¡¬í”„íŠ¸: {preview}", log_extra)

            if await db_utils.check_api_rate_limit(self.bot.db, limit_key, rpm, rpd):
                self._debug(f"[Gemini:{model.model_name}] í˜¸ì¶œ ì°¨ë‹¨ - rate limit ë„ë‹¬ ({limit_key})", log_extra)
                logger.warning(f"Gemini API í˜¸ì¶œ ì œí•œ({limit_key})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.", extra=log_extra)
                return None

            response = await model.generate_content_async(
                prompt,
                generation_config=generation_config,
                safety_settings=config.GEMINI_SAFETY_SETTINGS,
            )
            await db_utils.log_api_call(self.bot.db, limit_key)
            if self.debug_enabled and response is not None:
                text = getattr(response, "text", None)
                self._debug(
                    f"[Gemini:{model.model_name}] ì‘ë‹µ ìš”ì•½: {self._truncate_for_debug(text)}",
                    log_extra,
                )
            return response
        except Exception as e:
            logger.error(f"Gemini ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}", extra=log_extra, exc_info=True)
            return None

    async def _cometapi_generate_content(
        self,
        system_prompt: str,
        user_prompt: str,
        log_extra: dict,
        model: str | None = None,
    ) -> str | None:
        """CometAPI(OpenAI í˜¸í™˜)ë¥¼ í†µí•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
            log_extra: ë¡œê¹…ìš© ì¶”ê°€ ì •ë³´
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸, ì‹¤íŒ¨ ì‹œ None
        """
        if not self.cometapi_client:
            logger.warning("CometAPI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", extra=log_extra)
            return None

        try:
            if self.debug_enabled:
                self._debug(f"[CometAPI] system={self._truncate_for_debug(system_prompt)}", log_extra)
                self._debug(f"[CometAPI] user={self._truncate_for_debug(user_prompt)}", log_extra)

            completion = await self.cometapi_client.chat.completions.create(
                model=model or config.COMETAPI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=2048, # ì•½ê°„ ëŠ˜ë¦¼
                temperature=config.AI_TEMPERATURE,
                frequency_penalty=config.AI_FREQUENCY_PENALTY,
                presence_penalty=config.AI_PRESENCE_PENALTY,
            )

            response_text = completion.choices[0].message.content
            reasoning_text = getattr(completion.choices[0].message, 'reasoning_content', None)
            
            # [Debug] ì‘ë‹µ ë‚´ìš© í™•ì¸ì„ ìœ„í•œ ê°•ì œ ë¡œê¹…
            logger.info(f"[CometAPI Debug] Raw Response: {response_text!r}", extra=log_extra)
            try:
                # model_dump()ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸ (Pydantic v2)
                logger.info(f"[CometAPI Debug] Message Obj: {completion.choices[0].message}", extra=log_extra)
            except:
                pass

            await db_utils.log_api_call(self.bot.db, "cometapi")

            # ë§Œì•½ contentê°€ ë¹„ì–´ìˆëŠ”ë° reasoning_contentê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ë°˜í™˜ (Thinking ëª¨ë¸ ëŒ€ì‘)
            final_response = response_text
            if not final_response and reasoning_text:
                logger.warning("[CometAPI] Content is empty but reasoning_content exists. Using reasoning as fallback.", extra=log_extra)
                final_response = f"Thinking Process:\n{reasoning_text}" # í˜¹ì€ ê·¸ëƒ¥ reasoning_text

            if self.debug_enabled:
                self._debug(f"[CometAPI] ì‘ë‹µ: {self._truncate_for_debug(final_response)}", log_extra)

            return final_response.strip() if final_response else None

        except Exception as e:
            logger.error(f"CometAPI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", extra=log_extra, exc_info=True)
            return None

    async def _generate_local_embedding(self, content: str, log_extra: dict, prefix: str = "") -> np.ndarray | None:
        """SentenceTransformer ê¸°ë°˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not config.AI_MEMORY_ENABLED:
            return None
        if np is None:
            logger.warning("numpyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ AI ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", extra=log_extra)
            return None

        embedding = await get_embedding(content, prefix=prefix)
        if embedding is None:
            logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨", extra=log_extra)
        return embedding

    async def add_message_to_history(self, message: discord.Message):
        """AI í—ˆìš© ì±„ë„ì˜ ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ DBì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            message (discord.Message): Discord ì›ë³¸ ë©”ì‹œì§€.

        Notes:
            ë©”ì‹œì§€ê°€ ì¶©ë¶„íˆ ê¸¸ë©´ ì„ë² ë”© ìƒì„±ì„ ë¹„ë™ê¸° íƒœìŠ¤í¬ë¡œ ì˜ˆì•½í•©ë‹ˆë‹¤.
        """
        if not self.is_ready or not config.AI_MEMORY_ENABLED: return

        guild_id = message.guild.id if message.guild else 0
        
        # Guildì¸ ê²½ìš°ì—ë§Œ ì±„ë„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬
        if message.guild:
            try:
                channel_config = config.CHANNEL_AI_CONFIG.get(message.channel.id, {})
                if not channel_config.get("allowed", False): return
            except AttributeError:
                pass # message.channel has no id? rare.

        try:
            await self.bot.db.execute(
                "INSERT INTO conversation_history (message_id, guild_id, channel_id, user_id, user_name, content, is_bot, created_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                (
                    message.id,
                    guild_id,
                    message.channel.id,
                    message.author.id,
                    message.author.display_name,
                    message.content,
                    message.author.bot,
                    message.created_at.isoformat(),
                ),
            )
            await self._update_conversation_windows(message)
            await self.bot.db.commit()
            # ë‹¨ì¼ ë©”ì‹œì§€ ì„ë² ë”© ìƒì„± ë¡œì§ ì œê±° (ìœˆë„ìš° ê¸°ë°˜ ì„ë² ë”©ìœ¼ë¡œ ì „í™˜)
            # if not message.author.bot and message.content.strip():
            #     asyncio.create_task(self._create_and_save_embedding(message))
        except Exception as e:
            logger.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ DB ì˜¤ë¥˜: {e}", exc_info=True, extra={'guild_id': guild_id})

    async def _summarize_content(self, text: str) -> str:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìš©ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. DeepSeek ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆì„ ìµœì í™”í•©ë‹ˆë‹¤."""
        # [Optimization] í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´(400ì ë¯¸ë§Œ) ìš”ì•½í•˜ì§€ ì•Šê³  ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # (E5 ëª¨ë¸ì˜ 512 í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì•ˆì „í•œ ê¸¸ì´ë¡œ ì„¤ì •)
        if len(text) < 400:
            return text

        if not self.use_cometapi:
            # CometAPIê°€ êº¼ì ¸ìˆë‹¤ë©´ ì›ë³¸ ë°˜í™˜
            return text
        
        # [Optimization] ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í† í° ì ˆì•½
        safe_text = text[:4000] 
        
        try:
            # [Optimization] ê²€ìƒ‰(RAG) í’ˆì§ˆì„ ìœ„í•œ ìƒì„¸ ìš”ì•½ í”„ë¡¬í”„íŠ¸
            # E5 ì„ë² ë”© í•œê³„(512í† í°) ë‚´ì— ì¤‘ìš” ì •ë³´ê°€ ë‹¤ ë“¤ì–´ê°€ë„ë¡ 500ì ì œí•œ ë‘ 
            system_prompt = (
                "ë„ˆëŠ” ëŒ€í™” ë‚´ìš©ì„ ë‚˜ì¤‘ì— ê²€ìƒ‰í•˜ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ëŠ” 'ê¸°ì–µ ê´€ë¦¬ì'ì•¼.\n"
                "ì£¼ì–´ì§„ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ì— ë§ì¶° ìš”ì•½í•´.\n\n"
                "1. **ìƒí™© ì„¤ëª…**: ì–´ë–¤ ì£¼ì œë¡œ ëˆ„ê°€ ë¬´ìŠ¨ ë§ì„ í–ˆëŠ”ì§€ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ  (ë¶„ëŸ‰ ì œí•œ ì—†ìŒ, ìì„¸í• ìˆ˜ë¡ ì¢‹ìŒ)\n"
                "2. **ë¶„ìœ„ê¸°**: ëŒ€í™”ê°€ ì¦ê±°ì› ëŠ”ì§€, ì§„ì§€í–ˆëŠ”ì§€, í™”ê°€ ë‚¬ëŠ”ì§€ ë“± ê°ì • ìƒíƒœ ê¸°ë¡\n"
                "3. **í•µì‹¬ í‚¤ì›Œë“œ**: ë‚ ì§œ, ì‹œê°„, ì¥ì†Œ, URL, ì£¼ì‹ ì¢…ëª©, ì‚¬ëŒ ì´ë¦„ ë“± ê²€ìƒ‰ì— ê±¸ë ¤ì•¼ í•  ë‹¨ì–´ë“¤ì„ ë¹ ì§ì—†ì´ ë‚˜ì—´\n\n"
                "â€» **ì£¼ì˜ì‚¬í•­**: ì „ì²´ ìš”ì•½ ê¸¸ì´ëŠ” ë°˜ë“œì‹œ **500ì ì´ë‚´**ê°€ ë˜ë„ë¡ ë‚´ìš©ì„ í•µì‹¬ ìœ„ì£¼ë¡œ ì••ì¶•í•´. (ì„ë² ë”© ìš©ëŸ‰ ì œí•œ)"
            )
            user_prompt = f"--- ëŒ€í™” ë‚´ìš© ---\n{safe_text}"
            
            # max_tokens ì„¤ì •
            summary = await self._cometapi_generate_content(
                system_prompt, 
                user_prompt, 
                log_extra={'mode': 'rag_summary'}
            )
            
            if summary:
                return summary.strip()
            return text
        except Exception:
            return text

    async def _create_window_embedding(self, guild_id: int, channel_id: int, payload: list[dict[str, Any]]):
        """ëŒ€í™” ìœˆë„ìš°(ì²­í¬)ë¥¼ ì„ë² ë”©í•˜ì—¬ ë¡œì»¬ DBì— ì €ì¥í•©ë‹ˆë‹¤ (E5 passage prefix ì ìš©)."""
        if not payload:
            return

        # 1. ì²­í¬ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        merged_lines = []
        if payload and payload[0].get('created_at'):
            merged_lines.append(f"[ëŒ€í™” ì‹œê°„: {payload[0]['created_at']}]")
        
        prev_user = None
        current_block = []
        
        for p in payload:
            user = p.get('user_name', 'Unknown')
            content = p.get('content', '')
            
            if user == prev_user:
                current_block.append(content)
            else:
                if prev_user:
                    merged_content = " ".join(current_block)
                    merged_lines.append(f"{prev_user}: {merged_content}")
                prev_user = user
                current_block = [content]
        
        if prev_user:
            merged_content = " ".join(current_block)
            merged_lines.append(f"{prev_user}: {merged_content}")
            
        chunk_text = "\n".join(merged_lines)
        
        # [NEW] ìš”ì•½ ìƒì„± (ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ)
        summary_text = await self._summarize_content(chunk_text)
        embedding_text = f"passage: {summary_text}"
        
        # 2. ë©”íƒ€ë°ì´í„° ê²°ì • (ë§ˆì§€ë§‰ ë©”ì‹œì§€ ê¸°ì¤€)
        last_msg = payload[-1]
        message_id = last_msg['message_id']
        timestamp = last_msg['created_at']
        user_id = last_msg['user_id']
        
        log_extra = {'guild_id': guild_id, 'channel_id': channel_id, 'window_id': message_id}

        # 3. ì„ë² ë”© ìƒì„± (Summary ê¸°ë°˜)
        embedding_vector = await self._generate_local_embedding(
            embedding_text, 
            log_extra, 
            prefix="" # ì´ë¯¸ ìœ„ì—ì„œ passage: ë¶™ì„ (í˜¹ì€ _generateì— ë§¡ê¸°ë ¤ë©´ ìœ„ì—ì„œ ì œê±°)
        )
        # _generate_local_embedding ë‚´ë¶€ì—ì„œ prefix ì¸ìê°€ ìˆìœ¼ë©´ ë¶™ì„.
        # ì—¬ê¸°ì„œëŠ” ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì¸ì ì „ë‹¬ ë°©ì‹ì„ ì¡°ì •í•´ì•¼ í•¨.
        # ê¸°ì¡´ ì½”ë“œ: prefix="passage: " ì „ë‹¬í•¨.
        # ìˆ˜ì •: embedding_textì— ì´ë¯¸ passageë¥¼ ë¶™ì˜€ìœ¼ë¯€ë¡œ, prefixëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ.
        
        if embedding_vector is None:
            return

        # 4. DB ì €ì¥
        try:
            # message ì»¬ëŸ¼ì— 'ì²­í¬ ì „ì²´ í…ìŠ¤íŠ¸'ë¥¼ ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ì‹œ ì›ë³¸ ë¬¸ë§¥ ì œê³µ
            await self.discord_embedding_store.upsert_message_embedding(
                message_id=message_id,
                server_id=guild_id,
                channel_id=channel_id,
                user_id=user_id,
                user_name="Conversation Summary",  # ìš”ì•½ë³¸ì„ì„ ëª…ì‹œ
                message=f"ğŸ“Œ [ìš”ì•½] {summary_text}\n\n{chunk_text}", # ìš”ì•½ + ì›ë³¸ ì €ì¥
                timestamp_iso=timestamp,
                embedding=embedding_vector,
            )
        except Exception as e:
            logger.error(f"ì„ë² ë”© DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", extra=log_extra, exc_info=True)

    async def _update_conversation_windows(self, message: discord.Message) -> None:
        """ëŒ€í™” ìŠ¬ë¼ì´ë”© ìœˆë„ìš°(6ê°œ, stride=3)ë¥¼ ëˆ„ì í•´ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if self.bot.db is None:
            return

        guild_id = message.guild.id if message.guild else 0
        window_size = max(1, getattr(config, "CONVERSATION_WINDOW_SIZE", 6))
        stride = max(1, getattr(config, "CONVERSATION_WINDOW_STRIDE", 3))
        key = (guild_id, message.channel.id)

        # ì±„ë„ë³„ ìŠ¬ë¼ì´ë”© ë²„í¼ì— ë©”ì‹œì§€ë¥¼ ëˆ„ì í•œë‹¤.
        buffer = self._window_buffers.setdefault(key, deque(maxlen=window_size))
        entry = {
            "message_id": int(message.id),
            "user_id": int(message.author.id),
            "user_name": message.author.display_name or message.author.name or str(message.author.id),
            "content": (message.content or "").strip(),
            "is_bot": bool(message.author.bot),
            "created_at": message.created_at.isoformat(),
        }
        buffer.append(entry)

        # stride ê³„ì‚°ì„ ìœ„í•´ ì±„ë„ë³„ ì‚½ì… íšŸìˆ˜ë¥¼ ê¸°ë¡í•œë‹¤.
        counter = self._window_counts.get(key, 0) + 1
        self._window_counts[key] = counter

        # [Feature] ë©”ì‹œì§€ ê¸¸ì´ í•©ê³„ë¥¼ ê³„ì‚°í•˜ì—¬ í† í° ì œí•œì— ëŒ€ë¹„í•œë‹¤.
        total_chars = sum(len(item["content"]) for item in buffer)
        max_chars = getattr(config, "CONVERSATION_WINDOW_MAX_CHARS", 3000)

        # ìœˆë„ìš°ê°€ ê°€ë“ ì°¼ê±°ë‚˜, ë¬¸ìì—´ ê¸¸ì´ê°€ ì œí•œì„ ì´ˆê³¼í•˜ë©´ ì €ì¥ì„ ì‹œë„í•œë‹¤.
        is_full = len(buffer) >= window_size
        is_heavy = total_chars >= max_chars
        
        if not is_full and not is_heavy:
            return

        # stride ê°„ê²©ì— ë§ì¶° ìœˆë„ìš°ë¥¼ ì €ì¥í•œë‹¤.
        # ë‹¨, is_heavy(ìš©ëŸ‰ ì´ˆê³¼)ì¸ ê²½ìš°ì—ëŠ” strideì™€ ë¬´ê´€í•˜ê²Œ ì¦‰ì‹œ ì €ì¥í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ëˆ„ë½ì„ ë°©ì§€í•œë‹¤.
        if not is_heavy and (counter - window_size) % stride != 0:
            return
        
        # [Log] ìš©ëŸ‰ ì´ˆê³¼ë¡œ ì¸í•œ ê°•ì œ ì €ì¥ ì•Œë¦¼
        if is_heavy and not is_full:
            logger.info(f"ëŒ€í™” ìœˆë„ìš° ìš©ëŸ‰ ì´ˆê³¼({total_chars}ì)ë¡œ ì¦‰ì‹œ ì €ì¥: {message.channel.id}", extra={'guild_id': guild_id})

        try:
            payload = list(buffer)
            await self.bot.db.execute(
                """
                INSERT OR REPLACE INTO conversation_windows (
                    guild_id, channel_id, start_message_id, end_message_id,
                    message_count, messages_json, anchor_timestamp
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    guild_id,
                    message.channel.id,
                    payload[0]["message_id"],
                    payload[-1]["message_id"],
                    len(payload),
                    json.dumps(payload, ensure_ascii=False),
                    payload[-1]["created_at"],
                ),
            )
            # ìœˆë„ìš°ê°€ ì €ì¥ë  ë•Œ í•´ë‹¹ ìœˆë„ìš°ì— ëŒ€í•œ ì„ë² ë”©ë„ ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬)
            asyncio.create_task(
                self._create_window_embedding(guild_id, message.channel.id, payload)
            )
        except Exception as exc:  # pragma: no cover - ë°©ì–´ì  ë¡œê¹…
            logger.error(
                "ëŒ€í™” ìœˆë„ìš° ì €ì¥ ì¤‘ DB ì˜¤ë¥˜: %s",
                exc,
                extra={"guild_id": guild_id, "channel_id": message.channel.id},
                exc_info=True,
            )

    # ========== ìŠ¤ë§ˆíŠ¸ ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ (Google Custom Search API ì‚¬ìš©) ==========

    _WEB_SEARCH_TRIGGER_KEYWORDS = frozenset([
        'ì˜¤ëŠ˜', 'ìµœê·¼', 'ë‰´ìŠ¤', 'í˜„ì¬', 'ì§€ê¸ˆ', 'ì‹¤ì‹œê°„', 'ìµœì‹ ',
        'ì–´ì œ', 'ì´ë²ˆ ì£¼', 'ì´ë²ˆ ë‹¬', 'ì˜¬í•´', 'ê°€ê²©', 'ì‹œì„¸',
        'ì–¸ì œ', 'ë¬´ìŠ¨ ì¼', 'ë­” ì¼', 'ì–´ë–»ê²Œ', 'ë°©ë²•',
        'ì°¾ì•„', 'ê²€ìƒ‰', 'ì•Œë ¤ì¤˜', 'ë­ì•¼', 'ë¬´ì—‡', 'ì™œ'
    ])

    _NO_SEARCH_PATTERNS = frozenset([
        'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ë§ˆì‚¬ëª½', 'ë§ˆì‚¬ëª¨', 'ì„œë²„',
        'ì•„ê¹Œ', 'ì „ì—', 'ì§€ë‚œë²ˆ', 'ê¸°ì–µ', 'í–ˆì—ˆ', 'ë§í–ˆ'
    ])

    async def _should_use_web_search(self, query: str, rag_top_score: float) -> bool:
        """ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        
        ì¼ì¼ 100íšŒ ì œí•œì„ ê³ ë ¤í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
        """
        query_lower = query.lower()

        # RAG ì ìˆ˜ê°€ ì¶©ë¶„íˆ ë†’ìœ¼ë©´ ê²€ìƒ‰ ë¶ˆí•„ìš”
        if rag_top_score >= config.RAG_STRONG_SIMILARITY_THRESHOLD:
            return False

        # ì´ë¯¸ ë‹¤ë¥¸ ë„êµ¬(ë‚ ì”¨, ì£¼ì‹ ë“±)ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì§ˆë¬¸ì€ ì œì™¸
        if any(kw in query_lower for kw in self._WEATHER_KEYWORDS):
            return False
        if any(kw in query_lower for kw in self._STOCK_US_KEYWORDS | self._STOCK_KR_KEYWORDS):
            return False
        if any(kw in query_lower for kw in self._PLACE_KEYWORDS):
            return False

        # ë‚´ë¶€ ì •ë³´ë¡œ í•´ê²° ê°€ëŠ¥í•œ íŒ¨í„´ ì œì™¸
        if any(pat in query_lower for pat in self._NO_SEARCH_PATTERNS):
            return False

        # ì›¹ ê²€ìƒ‰ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œê°€ ìˆì–´ì•¼ ê²€ìƒ‰ ìˆ˜í–‰
        if not any(kw in query_lower for kw in self._WEB_SEARCH_TRIGGER_KEYWORDS):
            return False

        # ì¼ì¼ ì œí•œ í™•ì¸
        if await self._check_daily_search_limit():
            return False

        return True

    async def _check_daily_search_limit(self) -> bool:
        """Google Custom Search API ì¼ì¼ ì‚¬ìš©ëŸ‰ì´ ì œí•œì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        if not self.bot.db:
            return True  # DB ì—†ìœ¼ë©´ ê²€ìƒ‰ ë¹„í™œì„±í™”

        today_count = await db_utils.get_daily_api_count(self.bot.db, 'google_custom_search')
        limit = getattr(config, 'GOOGLE_CUSTOM_SEARCH_DAILY_LIMIT', 100)
        if today_count >= limit:
            logger.warning(f"Google Custom Search API ì¼ì¼ ì œí•œ({limit})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬: {today_count}")
            return True
        return False

    async def _generate_search_keywords(self, user_query: str, log_extra: dict) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        keyword_prompt = f"""[í˜„ì¬ ì‹œê°„]: {db_utils.get_current_time()}

ì‚¬ìš©ì ì§ˆë¬¸ì„ Google ê²€ìƒ‰ì— ì í•©í•œ í‚¤ì›Œë“œë¡œ ë³€í™˜í•´ì¤˜.

ê·œì¹™:
- í•œêµ­ì–´ ì§ˆë¬¸ì´ë©´ í•œêµ­ì–´ í‚¤ì›Œë“œ ìœ ì§€
- í•µì‹¬ ë‹¨ì–´ë§Œ ì¶”ì¶œ (ì¡°ì‚¬, ì–´ë¯¸ ì œê±°)
- ìµœëŒ€ 5ê°œ ë‹¨ì–´
- 'ìš”ì¦˜', 'ìµœê·¼' ë“±ì˜ ì‹œê°„ í‘œí˜„ì´ ìˆìœ¼ë©´ [í˜„ì¬ ì‹œê°„]ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ ì—°ë„ë‚˜ ì›”ì„ í‚¤ì›Œë“œì— í¬í•¨í•  ê²ƒ (ì˜ˆ: 2026ë…„ 1ì›”)
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ ë‚˜ì˜¤ë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
ê²€ìƒ‰ í‚¤ì›Œë“œ:"""

        keywords = None
        if self.use_cometapi:
            keywords = await self._cometapi_generate_content(
                "ë„ˆëŠ” ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± ì „ë¬¸ê°€ì•¼. ì…ë ¥ëœ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¡œ ë³€í™˜í•´. í‚¤ì›Œë“œë§Œ ì¶œë ¥í•´.",
                keyword_prompt,
                log_extra,
            )
        elif self.gemini_configured and genai:
            model = genai.GenerativeModel(config.AI_INTENT_MODEL_NAME)
            response = await self._safe_generate_content(model, keyword_prompt, log_extra)
            keywords = response.text.strip() if response and response.text else None

        if not keywords:
            # LLM ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            return self._extract_simple_keywords(user_query)

        return keywords.strip()

    def _extract_simple_keywords(self, query: str) -> str:
        """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (LLM í´ë°±ìš©)"""
        stopwords = {'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 
                     'í•´ì¤˜', 'ì•Œë ¤ì¤˜', 'ë­ì•¼', 'ë­”ê°€', 'ì¢€', 'ê·¸', 'ì €', 'ì´ê±°', 'ë­', 'ì–´ë–»ê²Œ'}
        words = query.split()
        keywords = [w for w in words if w not in stopwords and len(w) > 1]
        return ' '.join(keywords[:5])

    async def _generate_image_prompt(
        self,
        user_query: str,
        log_extra: dict,
        rag_context: str | None = None,
    ) -> str | None:
        """ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ìµœì í™”ëœ ì˜ë¬¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:
        - ì£¼ì œ(Subject) + ìŠ¤íƒ€ì¼(Style) + í’ˆì§ˆ íƒœê·¸(Quality) + ì¡°ëª…(Lighting) + êµ¬ë„(Composition)
        
        Args:
            user_query: ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­
            log_extra: ë¡œê¹…ìš© ì¶”ê°€ ì •ë³´
            rag_context: RAG ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì , ì„ ì •ì  ë‚´ìš© í¬í•¨ ì‹œ ë¬´ì‹œë¨)
            
        Returns:
            ì˜ë¬¸ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë˜ëŠ” None
        """
        # RAG ì»¨í…ìŠ¤íŠ¸ ì•ˆì „ì„± ê²€ì‚¬ (ì„ ì •ì  ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¬´ì‹œ)
        safe_context = ""
        if rag_context:
            # ì—„ê²©í•œ í•„í„°ë§: NSFW í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ RAG ì „ì²´ ë¬´ì‹œ
            rag_lower = rag_context.lower()
            nsfw_keywords = [
                'ì•¼í•œ', 'ì„ ì •ì ', 'ë…¸ì¶œ', 'ì„±ì¸', 'ìŒë€', 'ì—ë¡œ', 'ì„¹ì‹œ', 'ì•¼ë™',
                'nsfw', 'nude', 'naked', 'sexy', 'erotic', 'xxx', 'porn',
                'ë²—ì€', 'ì•Œëª¸', 'ë‚˜ì²´', 'ê°€ìŠ´', 'ì—‰ë©ì´', '19ê¸ˆ', '18ê¸ˆ'
            ]
            if not any(kw in rag_lower for kw in nsfw_keywords):
                safe_context = f"\n\n[Context from previous conversations - use if relevant]:\n{rag_context[:400]}"
        
        # ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """You are an expert AI image prompt engineer specializing in FLUX and Stable Diffusion models.
Your task: Convert the user's Korean image request into a HIGH-QUALITY English prompt.

## Prompt Structure (use this order):
1. **Subject**: Main subject with specific details (who/what, appearance, pose, expression)
2. **Environment/Background**: Setting, location, atmosphere
3. **Style**: Art style (photorealistic, anime, oil painting, digital art, watercolor, etc.)
4. **Lighting**: Lighting conditions (golden hour, studio lighting, dramatic shadows, soft ambient)
5. **Quality Tags**: Add these for better results: "masterpiece, best quality, highly detailed, 8k, ultra HD"
6. **Composition**: Camera angle, framing (close-up, wide shot, portrait, etc.)

## Example Output:
"A fluffy orange tabby cat sitting on a windowsill, looking outside at falling snow, cozy indoor lighting, photorealistic style, soft warm lighting from the window, masterpiece, best quality, highly detailed, 8k, shallow depth of field, medium shot"

## CRITICAL SAFETY RULES (RELAXED):
- ONLY SUBSTITUTE if the request specifically asks for: EXPLICIT SEXUAL ACTS, GENITALIA, or NUDITY (NIPPLES/GENITALS EXPOSED).
- ALLOW: Swimwear, lingerie, attractive characters, sexy poses, celebrities, real people, action violence, horror, and hate symbols are ALL ALLOWED.
- IF and ONLY IF the request is EXPLICIT PORNOGRAPHY (e.g. sex acts, exposed genitalia): SUBSTITUTE with: "A beautiful serene landscape with mountains and a peaceful lake, golden hour lighting, photorealistic, masterpiece, best quality, 8k"
- Do NOT explain why you substituted - just output the safe alternative

## Output Rules:
- Output ONLY the English prompt, nothing else
- No Korean text in the output
- No explanations, no "Prompt:" prefix, just the raw prompt
- Length: 50-150 words optimal"""

        user_prompt = f"""User's request (in Korean): {user_query}{safe_context}

Generate the optimized English image prompt:"""

        image_prompt = None
        
        if self.use_cometapi:
            image_prompt = await self._cometapi_generate_content(
                system_prompt,
                user_prompt,
                log_extra,
            )
            
            # CometAPI ê²°ê³¼ì— í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬ (ì¬ì‹œë„ ìœ ë„)
            if image_prompt and any('\uac00' <= char <= '\ud7a3' for char in image_prompt):
                logger.warning(f"CometAPI ìƒì„± í”„ë¡¬í”„íŠ¸ì— í•œêµ­ì–´ í¬í•¨ë¨, ì‹¤íŒ¨ ì²˜ë¦¬: {image_prompt}", extra=log_extra)
                image_prompt = None
            
        # CometAPI ì‹¤íŒ¨/í•œêµ­ì–´í¬í•¨ ë˜ëŠ” ë¹„í™œì„±í™” ì‹œ Gemini í´ë°±
        if not image_prompt and self.gemini_configured and genai:
            if self.use_cometapi: # CometAPI ì‹œë„ í›„ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ë¡œê·¸
                logger.info("CometAPI ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨(ë˜ëŠ” í•œêµ­ì–´ í¬í•¨), Geminië¡œ ì‹œë„í•©ë‹ˆë‹¤.", extra=log_extra)
            model = genai.GenerativeModel(config.AI_INTENT_MODEL_NAME)
            response = await self._safe_generate_content(model, user_prompt, log_extra)
            image_prompt = response.text.strip() if response and response.text else None
        
        if image_prompt:
            # í”„ë¡¬í”„íŠ¸ ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´/ì„¤ëª… ì œê±°)
            image_prompt = image_prompt.strip()
            
            # ì ‘ë‘ì‚¬ ì œê±°
            prefixes_to_remove = [
                "Prompt:", "prompt:", "Image prompt:", "Output:", 
                "English prompt:", "Here is", "Here's", "The prompt is:"
            ]
            for prefix in prefixes_to_remove:
                if image_prompt.lower().startswith(prefix.lower()):
                    image_prompt = image_prompt[len(prefix):].strip()
            
            # ë”°ì˜´í‘œ ì œê±°
            if (image_prompt.startswith('"') and image_prompt.endswith('"')) or \
               (image_prompt.startswith("'") and image_prompt.endswith("'")):
                image_prompt = image_prompt[1:-1]
            
            # ë§ˆì§€ë§‰ ì•ˆì „ ê²€ì‚¬: í˜¹ì‹œ ì—¬ì „íˆ í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ í•œêµ­ì–´ë§Œ ì œê±° ì‹œë„
            if any('\uac00' <= char <= '\ud7a3' for char in image_prompt):
                logger.warning("ìµœì¢… í”„ë¡¬í”„íŠ¸ì— í•œêµ­ì–´ê°€ í¬í•¨ë¨. í•œêµ­ì–´ ë¬¸ì ì œê±° ì‹œë„.", extra=log_extra)
                # í•œêµ­ì–´ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ì œê±° (ê°€-í£)
                image_prompt = re.sub(r'[\uac00-\ud7a3]+', '', image_prompt).strip()
                # ì œê±° í›„ ë¹ˆ ë¬¸ìì—´ì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                if not image_prompt:
                    logger.warning("í•œêµ­ì–´ ì œê±° í›„ í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆìŒ. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©.", extra=log_extra)
                    image_prompt = "A beautiful serene landscape with mountains and a peaceful lake at sunset, golden hour lighting, photorealistic, masterpiece, best quality, highly detailed, 8k, wide angle shot"
            
            self._debug(f"[ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸] ìƒì„±ë¨: {self._truncate_for_debug(image_prompt)}", log_extra)
            return image_prompt
        
        logger.warning("ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨", extra=log_extra)
        return None


    async def _execute_web_search_with_llm(
        self,
        user_query: str,
        log_extra: dict
    ) -> dict:
        """Google Custom Search API í˜¸ì¶œ í›„ LLMìœ¼ë¡œ ê²°ê³¼ë¥¼ í•´ì„í•©ë‹ˆë‹¤.

        í”Œë¡œìš°:
        1. LLMì´ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        2. Google Custom Search API í˜¸ì¶œ (tools_cog.web_search ì‚¬ìš©)
        3. LLMì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì½ê³  ë‹µë³€ ìƒì„±ìš© ìš”ì•½ ë°˜í™˜
        """
        # 1. ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        search_keywords = await self._generate_search_keywords(user_query, log_extra)
        self._debug(f"[ì›¹ê²€ìƒ‰] ìƒì„±ëœ í‚¤ì›Œë“œ: {search_keywords}", log_extra)

        # 2. tools_cog.web_search í˜¸ì¶œ (ì´ë¯¸ Google CSE ì—°ë™ë¨)
        if not self.tools_cog:
            return {"error": "ToolsCogê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        search_result = await self.tools_cog.web_search(search_keywords)

        # 3. ê²€ìƒ‰ ê²°ê³¼ ê¸°ë¡
        await db_utils.log_api_call(self.bot.db, 'google_custom_search')

        if not search_result or 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤' in search_result:
            return {"result": None, "error": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", "search_keywords": search_keywords}

        # 4. LLMìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ í•´ì„ ë° ìš”ì•½
        channel_id = log_extra.get('channel_id')
        persona_prompt = self._get_channel_system_prompt(channel_id)

        system_prompt = f"""ë„ˆëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê³  ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” AI ì—ì´ì „íŠ¸ì•¼.
ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , ì•„ë˜ í˜ë¥´ì†Œë‚˜ì— ë§ì¶°ì„œ ë„¤ ì£¼ê´€ì ì¸ ì˜ê²¬ì´ë‚˜ ê°ìƒì„ ì„ì–´ ì¹œêµ¬ì—ê²Œ ë§í•˜ë“¯ì´ ì„¤ëª…í•´ì¤˜.
ë°˜ë“œì‹œ ì•„ë˜ ì„¤ì •ëœ ë§íˆ¬ë¥¼ ì™„ë²½í•˜ê²Œ ìœ ì§€í•´ì•¼ í•´.

{persona_prompt}
"""

        summarize_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: '{user_query}'

ê²€ìƒ‰ ê²°ê³¼:
{search_result[:6000]}

ë‹µë³€ ê°€ì´ë“œ:
1. ê²€ìƒ‰ëœ ì •ë³´ì˜ í•µì‹¬ì„ ì •í™•íˆ ì „ë‹¬í•´.
2. í•˜ì§€ë§Œ ë§íˆ¬ëŠ” ìœ„ì—ì„œ ì„¤ì •ëœ í˜ë¥´ì†Œë‚˜ë¥¼ ì™„ë²½í•˜ê²Œ ìœ ì§€í•´ì•¼ í•´.
3. ë‹¨ìˆœ ì •ë³´ ë‚˜ì—´ ëŒ€ì‹  "ì™€, ì´ê±° ì§„ì§œ ì‹ ê¸°í•˜ë‹¤", "ì´ëŸ° ê²ƒë„ ìˆë„¤?", "ë„ì›€ì´ ëìœ¼ë©´ ì¢‹ê² ì–´" ê°™ì´ ë„¤ ê°ìƒì´ë‚˜ ë¦¬ì•¡ì…˜ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„ì–´ì¤˜.
4. ì¹œêµ¬ì—ê²Œ ì¹´í†¡í•˜ë“¯ì´ 3-4ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´.

ë‹µë³€:"""

        summary = None
        if self.use_cometapi:
            summary = await self._cometapi_generate_content(
                system_prompt,
                summarize_prompt,
                log_extra,
            )
        elif self.gemini_configured and genai:
            model = genai.GenerativeModel(config.AI_INTENT_MODEL_NAME)
            response = await self._safe_generate_content(model, summarize_prompt, log_extra)
            summary = response.text.strip() if response and response.text else None

        if summary:
            self._debug(f"[ì›¹ê²€ìƒ‰] ìš”ì•½ ê²°ê³¼: {self._truncate_for_debug(summary)}", log_extra)
            return {"result": summary, "summary": summary, "search_keywords": search_keywords}

        # LLM ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
        return {"result": search_result[:1500], "search_keywords": search_keywords}


    # ========== í‚¤ì›Œë“œ ê¸°ë°˜ ë„êµ¬ ê°ì§€ (Lite ëª¨ë¸ ëŒ€ì²´) ==========

    _WEATHER_KEYWORDS = frozenset(['ë‚ ì”¨', 'ê¸°ì˜¨', 'ì˜¨ë„', 'ë¹„', 'ëˆˆ', 'ë§‘', 'íë¦¼', 'ìš°ì‚°', 'ê°•ìˆ˜', 'ì¼ê¸°ì˜ˆë³´', 'ì²´ê°', 'ë¥', 'ì¶¥', 'ìŒ€ìŒ€', 'ë”°ëœ»', 'í­ì—¼', 'í•œíŒŒ', 'íƒœí’'])
    _STOCK_US_KEYWORDS = frozenset(['ì• í”Œ', 'apple', 'aapl', 'í…ŒìŠ¬ë¼', 'tesla', 'tsla', 'êµ¬ê¸€', 'google', 'googl', 'ì—”ë¹„ë””ì•„', 'nvidia', 'nvda', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'microsoft', 'msft', 'ì•„ë§ˆì¡´', 'amazon', 'amzn', 'ë§¥ë„ë‚ ë“œ', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì½”ì¹´ì½œë¼', 'í©ì‹œ', 'ë„·í”Œë¦­ìŠ¤', 'ë©”íƒ€', 'í˜ì´ìŠ¤ë¶', 'ë””ì¦ˆë‹ˆ', 'ì¸í…”', 'amd', 'ë‚˜ì´í‚¤', 'ì½”ìŠ¤íŠ¸ì½”', 'ë²„í¬ì…”'])
    _STOCK_KR_KEYWORDS = frozenset(['ì‚¼ì„±ì „ì', 'í˜„ëŒ€ì°¨', 'skí•˜ì´ë‹‰ìŠ¤', 'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤', 'lgì—ë„ˆì§€', 'ì…€íŠ¸ë¦¬ì˜¨', 'ì‚¼ì„±ë°”ì´ì˜¤', 'ê¸°ì•„', 'í¬ìŠ¤ì½”'])
    _STOCK_GENERAL_KEYWORDS = frozenset(['ì£¼ê°€', 'ì£¼ì‹', 'ì‹œì„¸', 'ì¢…ê°€', 'ì‹œê°€', 'ìƒì¥'])
    _PLACE_KEYWORDS = frozenset(['ë§›ì§‘', 'ì¹´í˜', 'ìŒì‹ì ', 'ì‹ë‹¹', 'ì¶”ì²œ', 'ê·¼ì²˜', 'ì£¼ë³€', 'ê°€ë³¼ë§Œí•œ', 'í•«í”Œ'])
    _LOCATION_KEYWORDS = [] # Deprecated: ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (DB ìºì‹œë¡œ ëŒ€ì²´)
    
    # ì´ë¯¸ì§€ ìƒì„± í‚¤ì›Œë“œ
    _IMAGE_GEN_KEYWORDS = frozenset([
        'ì´ë¯¸ì§€ ìƒì„±', 'ê·¸ë¦¼ ê·¸ë ¤', 'ì‚¬ì§„ ë§Œë“¤ì–´', 'ì´ë¯¸ì§€ ë§Œë“¤ì–´',
        'ê·¸ë ¤ì¤˜', 'ìƒì„±í•´ì¤˜', 'ê·¸ë¦¼ ìƒì„±', 'ì´ë¯¸ì§€ ê·¸ë ¤', 'ì‚¬ì§„ ìƒì„±',
        'ê·¸ë ¤ì¤˜', 'ë§Œë“¤ì–´ì¤˜', 'ê·¸ë¦¼ìœ¼ë¡œ', 'ì´ë¯¸ì§€ë¡œ', 
        'generate image', 'create image', 'draw', 'make an image',
    ])

    def _detect_tools_by_keyword(self, query: str) -> list[dict]:
        """í‚¤ì›Œë“œ íŒ¨í„´ìœ¼ë¡œ í•„ìš”í•œ ë„êµ¬ë¥¼ ê°ì§€í•©ë‹ˆë‹¤. Lite ëª¨ë¸ì„ ëŒ€ì²´í•©ë‹ˆë‹¤."""
        tools = []
        query_lower = query.lower()

        # ë‚ ì”¨ ê°ì§€
        if any(kw in query_lower for kw in self._WEATHER_KEYWORDS):
            location = self._extract_location_from_query(query) or 'ê´‘ì–‘'

            day_offset = 0
            if "ë‚´ì¼" in query:
                day_offset = 1
            elif "ëª¨ë ˆ" in query:
                day_offset = 2
            elif "ê¸€í”¼" in query:
                day_offset = 3
            elif any(kw in query for kw in ["ë‹¤ìŒì£¼", "ì´ë²ˆì£¼", "ì£¼ë§", "ì¼ì£¼ì¼"]):
                day_offset = 3 # Start of mid-term forecast

            tools.append({
                'tool_to_use': 'get_weather_forecast',
                'tool_name': 'get_weather_forecast',
                'parameters': {'location': location, 'day_offset': day_offset}
            })
            return tools  # ë‚ ì”¨ ìš”ì²­ì€ ë‹¨ì¼ ë„êµ¬ë¡œ ì²˜ë¦¬

        # [Refactor] Unified Stock Detection (yfinance + LLM Extraction)
        # í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, "ì£¼ê°€", "ì–¼ë§ˆ" ë“±ì˜ í‘œí˜„ì´ ìˆìœ¼ë©´ ì‹œë„
        stock_triggers = self._STOCK_US_KEYWORDS | self._STOCK_KR_KEYWORDS | self._STOCK_GENERAL_KEYWORDS
        if any(kw in query_lower for kw in stock_triggers) or "ì£¼ê°€" in query_lower or "ì£¼ì‹" in query_lower or "ì‹œì„¸" in query_lower:
             # LLMì„ í†µí•´ í‹°ì»¤ ì¶”ì¶œ ì‹œë„ (ê°•ë ¥í•œ ì¶”ì¶œê¸°)
             # ê¸°ì¡´ ë¡œì§ ëŒ€ì‹  ë°”ë¡œ LLMì— ì˜ì¡´í•˜ì—¬ ìœ ì—°ì„± í™•ë³´
             logger.info(f"ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€: '{query}' -> í‹°ì»¤ ì¶”ì¶œ ì‹œë„")
             
             # ë„êµ¬ í˜¸ì¶œ ê³„íšì—ëŠ” 'user_query'ë§Œ ë„˜ê¸°ê³ , ì‹¤ì œ ì‹¤í–‰ ì‹œì ì— extract_ticker_with_llm í˜¸ì¶œí•˜ë„ë¡ ë³€ê²½í•  ìˆ˜ë„ ìˆìœ¼ë‚˜,
             # ì—¬ê¸°ì„  ë„êµ¬ íŒŒë¼ë¯¸í„°ê°€ ëª…í™•í•´ì•¼ í•˜ë¯€ë¡œ, tool execution ë‹¨ê³„ì—ì„œ extractionì„ ìˆ˜í–‰í•˜ë„ë¡ 
             # 'get_stock_price' ë„êµ¬ì— ì¿¼ë¦¬ ìì²´ë¥¼ ë„˜ê¸°ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½ ì œì•ˆ.
             # ToolsCog.get_stock_priceê°€ (stock_name=...) ëŒ€ì‹  (query=...)ë¥¼ ë°›ì•„ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜,
             # ì•„ë‹ˆë©´ ì—¬ê¸°ì„œ ì¶”ì¶œí•´ì„œ ë„˜ê²¨ì•¼ í•¨. 
             # ì‹¤í–‰ ì†ë„ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ ì¶”ì¶œí•˜ì§€ ì•Šê³  ToolsCogì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ 'query'ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬.
             
             tools.append({
                'tool_to_use': 'get_stock_price',
                'tool_name': 'get_stock_price',
                'parameters': {'user_query': query} # stock_name ëŒ€ì‹  user_query ì „ë‹¬
             })
             return tools

        # ì¥ì†Œ ê²€ìƒ‰ ê°ì§€
        if any(kw in query_lower for kw in self._PLACE_KEYWORDS):
            # ìœ„ì¹˜ ì •ë³´ê°€ ìˆê³  ì¿¼ë¦¬ì— ì•„ì§ ì—†ìœ¼ë©´ ì¶”ê°€
            location = self._extract_location_from_query(query) or ''
            # ì´ë¯¸ ì¿¼ë¦¬ì— ìœ„ì¹˜ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            search_query = query if location in query else f"{location} {query}".strip()
            tools.append({
                'tool_to_use': 'search_for_place',
                'tool_name': 'search_for_place',
                'parameters': {'query': search_query}
            })
            return tools

        # ì´ë¯¸ì§€ ìƒì„± ê°ì§€ (CometAPI flux-2-flex)
        if any(kw in query_lower for kw in self._IMAGE_GEN_KEYWORDS):
            # ì´ë¯¸ì§€ ìƒì„±ì€ íŠ¹ë³„ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë¯€ë¡œ user_queryë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
            # AIê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , generate_image ë„êµ¬ë¥¼ í˜¸ì¶œ
            tools.append({
                'tool_to_use': 'generate_image',
                'tool_name': 'generate_image',
                'parameters': {'user_query': query}  # í”„ë¡¬í”„íŠ¸ ìƒì„± í•„ìš”
            })
            return tools

        # ë„êµ¬ í•„ìš” ì—†ìŒ - ì¼ë°˜ ëŒ€í™” ë˜ëŠ” RAGë¡œ ì²˜ë¦¬
        return tools

    def _extract_location_from_query(self, query: str) -> str | None:
        """ì¿¼ë¦¬ì—ì„œ ì§€ì—­ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (DB ìºì‹œ ì‚¬ìš©)."""
        # ìºì‹œê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¡œë“œ ì‹œë„ (ë™ê¸° ë©”ì„œë“œë¼ await ë¶ˆê°€í•˜ì§€ë§Œ, process_agentì—ì„œ ë¯¸ë¦¬ ë¡œë“œë¨ì„ ê°€ì •)
        # ë§Œì•½ ë¡œë“œ ì•ˆ ëœ ìƒíƒœë¼ë©´ ì–´ì©” ìˆ˜ ì—†ì´ pass
        
        # ê¸´ ì´ë¦„ë¶€í„° ë§¤ì¹­í•˜ì—¬ ì˜¤íƒì§€ ë°©ì§€ (ì˜ˆ: 'ë‚˜ì£¼ì‹œ' vs 'ë‚˜ì£¼')
        # ë§¤ë²ˆ ì •ë ¬í•˜ë©´ ëŠë¦¬ë¯€ë¡œ, ìºì‹œê°€ í´ ê²½ìš° ìµœì í™” í•„ìš”. ì¼ë‹¨ì€ ë‹¨ìˆœ ìˆœíšŒ.
        # ì„±ëŠ¥ì„ ìœ„í•´ ì¿¼ë¦¬ì— ìˆëŠ” ë‹¨ì–´ë§Œ í•„í„°ë§í•˜ëŠ” ë°©ì‹ì´ ì¢‹ìŒ.
        
        if not self.location_cache:
             return None

        # ì¿¼ë¦¬ê°€ ì§§ìœ¼ë©´ ê·¸ëƒ¥ ìˆœíšŒ
        # ë§¤ì¹­ëœ ê²ƒ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì„ ì„ íƒ
        best_match = None
        for location in self.location_cache:
            if location in query:
                if best_match is None or len(location) > len(best_match):
                    best_match = location
        
        return best_match

    def _extract_us_stock_symbol(self, query_lower: str) -> str | None:
        """ì¿¼ë¦¬ì—ì„œ ë¯¸êµ­ ì£¼ì‹ ì‹¬ë³¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        symbol_map = {
            'ì• í”Œ': 'AAPL', 'apple': 'AAPL', 'aapl': 'AAPL',
            'í…ŒìŠ¬ë¼': 'TSLA', 'tesla': 'TSLA', 'tsla': 'TSLA',
            'êµ¬ê¸€': 'GOOGL', 'google': 'GOOGL', 'googl': 'GOOGL',
            'ì—”ë¹„ë””ì•„': 'NVDA', 'nvidia': 'NVDA', 'nvda': 'NVDA',
            'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸': 'MSFT', 'microsoft': 'MSFT', 'msft': 'MSFT',
            'ì•„ë§ˆì¡´': 'AMZN', 'amazon': 'AMZN', 'amzn': 'AMZN',
            'ë§¥ë„ë‚ ë“œ': 'MCD', 'mcd': 'MCD',
            'ìŠ¤íƒ€ë²…ìŠ¤': 'SBUX', 'sbux': 'SBUX',
            'ì½”ì¹´ì½œë¼': 'KO', 'coca-cola': 'KO', 'ko': 'KO',
            'í©ì‹œ': 'PEP', 'pepsi': 'PEP',
            'ë„·í”Œë¦­ìŠ¤': 'NFLX', 'netflix': 'NFLX',
            'ë©”íƒ€': 'META', 'í˜ì´ìŠ¤ë¶': 'META', 'meta': 'META',
            'ë””ì¦ˆë‹ˆ': 'DIS', 'disney': 'DIS',
            'ì¸í…”': 'INTC', 'intel': 'INTC',
            'amd': 'AMD',
            'ë‚˜ì´í‚¤': 'NKE', 'nike': 'NKE',
            'ì½”ìŠ¤íŠ¸ì½”': 'COST', 'costco': 'COST',
            'ë²„í¬ì…”': 'BRK.B', 'berkshire': 'BRK.B'
        }
        for keyword, symbol in symbol_map.items():
            if keyword in query_lower:
                return symbol
        return None

    def _extract_kr_stock_ticker(self, query_lower: str) -> str | None:
        """ì¿¼ë¦¬ì—ì„œ í•œêµ­ ì£¼ì‹ ì¢…ëª© ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        ticker_map = {
            'ì‚¼ì„±ì „ì': '005930', 'í˜„ëŒ€ì°¨': '005380', 'skí•˜ì´ë‹‰ìŠ¤': '000660',
            'ë„¤ì´ë²„': '035420', 'ì¹´ì¹´ì˜¤': '035720', 'lgì—ë„ˆì§€': '373220',
            'ì…€íŠ¸ë¦¬ì˜¨': '068270', 'ì‚¼ì„±ë°”ì´ì˜¤': '207940', 'ê¸°ì•„': '000270', 'í¬ìŠ¤ì½”': '005490',
        }
        for keyword, ticker in ticker_map.items():
            if keyword in query_lower:
                return ticker
        return None

    async def _get_rag_context(
        self,
        guild_id: int,
        channel_id: int,
        user_id: int,
        query: str,
        recent_messages: list[str] | None = None,
    ) -> tuple[str, list[dict[str, Any]], float, list[str]]:
        """RAG: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        if not config.AI_MEMORY_ENABLED:
            return "", [], 0.0, []

        log_extra = {'guild_id': guild_id, 'channel_id': channel_id, 'user_id': user_id}
        logger.info("RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘. Query: '%s'", query, extra=log_extra)

        engine = getattr(self, "hybrid_search_engine", None)
        if engine is None:
            logger.warning("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", extra=log_extra)
            return "", [], 0.0, []

        # [NEW] DM(ê¸¸ë“œ ì—†ìŒ)ì¸ ê²½ìš°, ë´‡ì˜ ë‹µë³€ë„ ê¸°ì–µí•˜ê¸° ìœ„í•´ user_id í•„í„°ë¥¼ í•´ì œ(None)í•©ë‹ˆë‹¤.
        # DMì€ channel_idê°€ ì‚¬ìš©ìë³„ë¡œ ê³ ìœ í•˜ë¯€ë¡œ, ì±„ë„ IDë§Œìœ¼ë¡œë„ ë°ì´í„° ê²©ë¦¬ê°€ ë³´ì¥ë©ë‹ˆë‹¤.
        search_user_id = user_id if guild_id else None

        try:
            result = await engine.search(
                query,
                guild_id=guild_id,
                channel_id=channel_id,
                user_id=search_user_id,
                recent_messages=recent_messages,
            )
        except Exception as exc:
            logger.error("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: %s", exc, extra=log_extra, exc_info=True)
            return "", [], 0.0, []

        if not result.entries:
            logger.info("RAG: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", extra=log_extra)
            return "", [], 0.0, []

        limit = max(getattr(config, "RAG_HYBRID_TOP_K", 4), 1)
        threshold = getattr(config, "RAG_SIMILARITY_THRESHOLD", 0.6)
        prepared_entries: list[dict[str, Any]] = []
        rag_blocks: list[str] = []

        # í•­ìƒ RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥
        log_lines = []
        for entry in result.entries[:limit]:
            score = float(entry.get("combined_score", 0.0) or entry.get("score", 0.0) or 0.0)
            dialogue_block = (entry.get("dialogue_block") or entry.get("message") or "").strip()
            snippet = dialogue_block[:100] + "..." if len(dialogue_block) > 100 else dialogue_block
            
            # ì†ŒìŠ¤ íƒœê·¸ ê²°ì •: origin í•„ë“œ ë˜ëŠ” í˜•ì‹ìœ¼ë¡œ íŒë‹¨
            origin = entry.get("origin", "")
            if origin == "kakao" or "[Merged Context]" in snippet:
                source_tag = "[KAKAO]"
            elif origin == "discord" or "[" in snippet and "][2026-" in snippet:
                source_tag = "[DISCORD]"
            else:
                source_tag = "[UNKNOWN]"
            
            log_lines.append(f"  [{score:.3f}] {source_tag} {snippet}")

            # ì„ê³„ê°’ ì´í•˜ëŠ” ë¬´ì‹œ (ì“°ë ˆê¸°ê°’ í•„í„°ë§)
            if score < threshold:
                continue

            if not dialogue_block:
                continue

            rag_blocks.append(dialogue_block)
            prepared_entries.append(
                {
                    "dialogue_block": dialogue_block,
                    "combined_score": score,
                    "similarity": entry.get("similarity"),
                    "bm25_score": entry.get("bm25_score"),
                    "sources": entry.get("sources"),
                    "origin": entry.get("origin"),
                    "speaker": entry.get("speaker"),
                    "message_id": entry.get("message_id"),
                }
            )

        # í•­ìƒ ë¡œê·¸ ì¶œë ¥ (ì ìˆ˜ í¬í•¨)
        logger.info(
            "RAG ê²€ìƒ‰ ê²°ê³¼ (threshold=%.2f):\n%s",
            threshold,
            "\n".join(log_lines) if log_lines else "  (ì—†ìŒ)",
            extra=log_extra,
        )

        if not rag_blocks:
            logger.info("RAG: ì„ê³„ê°’(%.2f) ì´ìƒì˜ ê²°ê³¼ê°€ ì—†ì–´ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", threshold, extra=log_extra)
            return "", [], 0.0, []

        context_sections = []
        for idx, block in enumerate(rag_blocks, start=1):
            context_sections.append(f"[ëŒ€í™” {idx}]\n{block}")
        context_str = "\n\n".join(context_sections)

        top_score = float(result.top_score or 0.0)
        logger.info(
            "RAG: ì‚¬ìš©í•  ì»¨í…ìŠ¤íŠ¸ %dê°œ (ìµœê³  ì ìˆ˜=%.3f)",
            len(prepared_entries),
            top_score,
            extra=log_extra,
        )

        logger.debug("RAG ê²°ê³¼: %s", context_str, extra=log_extra)
        return context_str, prepared_entries, top_score, rag_blocks

    async def _collect_recent_search_messages(self, message: discord.Message, limit: int = 10) -> list[str]:
        """ìµœê·¼ ì±„ë„ ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©ì/ë´‡ ë°œí™”ë¥¼ ì¶”ì¶œí•´ ê²€ìƒ‰ í™•ì¥ì— ì‚¬ìš©í•©ë‹ˆë‹¤."""
        previous_user: str | None = None
        previous_bot: str | None = None
        async for msg in message.channel.history(limit=limit):
            if msg.id == message.id:
                continue
            content = (msg.content or "").strip()
            if not content:
                continue
            if previous_user is None and msg.author.id == message.author.id:
                previous_user = content  # ë°”ë¡œ ì´ì „ ì‚¬ìš©ìì˜ ì§ˆë¬¸
            elif previous_bot is None and getattr(msg.author, "bot", False):
                previous_bot = content  # ì§ì „ ë´‡ ë‹µë³€
            if previous_user and previous_bot:
                break

        collected: list[str] = []
        if previous_user:
            collected.append(previous_user)
        if previous_bot:
            collected.append(previous_bot)
        return collected

    @staticmethod
    def _extract_json_block(text: str) -> str:
        stripped = text.strip()
        if stripped.startswith("```"):
            stripped = re.sub(r'^```[a-zA-Z0-9_]*\s*', '', stripped)
            if stripped.endswith("```"):
                stripped = stripped[:-3]
        start = stripped.find('{')
        end = stripped.rfind('}')
        if start != -1 and end != -1 and end >= start:
            return stripped[start : end + 1]
        return stripped

    @staticmethod
    def _normalize_score(value: Any) -> float | None:
        if value is None:
            return None
        try:
            score = float(value)
        except (TypeError, ValueError):
            return None
        if score < 0.0:
            return 0.0
        if score > 1.0:
            return 1.0
        return score

    def _parse_thinking_response(self, text: str) -> dict[str, Any]:
        stripped = text.strip()
        data: Any | None = None
        for candidate in (stripped, self._extract_json_block(stripped)):
            if not candidate:
                continue
            try:
                data = json.loads(candidate)
                break
            except json.JSONDecodeError:
                continue

        if data is None:
            logger.warning("Thinking ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: ìœ íš¨í•œ JSON ë¸”ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {}

        if isinstance(data, list):
            plan: list[dict[str, Any]] = []
            for item in data:
                if not isinstance(item, dict):
                    continue
                tool_call = item.get("tool_call") or item
                if not isinstance(tool_call, dict):
                    continue
                tool_name = (
                    tool_call.get("tool_name")
                    or tool_call.get("tool_to_use")
                    or tool_call.get("function")
                )
                if not tool_name:
                    continue
                params = (
                    tool_call.get("parameters")
                    or tool_call.get("args")
                    or {}
                )
                if not isinstance(params, dict):
                    params = {}
                plan.append(
                    {
                        "tool_to_use": tool_name,
                        "tool_name": tool_name,
                        "parameters": params,
                    }
                )
            return {
                "analysis": "",
                "draft": "",
                "tool_plan": plan,
                "self_score": {},
                "needs_flash": bool(plan),
            }

        if not isinstance(data, dict):
            return {}

        analysis = str(data.get("analysis") or "").strip()
        draft = str(data.get("draft") or "").strip()

        plan: list[dict[str, Any]] = []
        raw_plan = data.get("tool_plan")
        if isinstance(raw_plan, list):
            for item in raw_plan:
                if not isinstance(item, dict):
                    continue
                tool_name = item.get("tool_name") or item.get("tool_to_use")
                if not tool_name:
                    continue
                parameters = item.get("parameters")
                if not isinstance(parameters, dict):
                    parameters = {}
                plan.append({
                    "tool_to_use": tool_name,
                    "tool_name": tool_name,
                    "parameters": parameters,
                })

        score_payload = data.get("self_score")
        scores: dict[str, float] = {}
        if isinstance(score_payload, dict):
            for key in ("accuracy", "completeness", "risk", "overall"):
                normalized = self._normalize_score(score_payload.get(key))
                if normalized is not None:
                    scores[key] = normalized

        needs_flash = bool(data.get("needs_flash"))

        return {
            "analysis": analysis,
            "draft": draft,
            "tool_plan": plan,
            "self_score": scores,
            "needs_flash": needs_flash,
        }

    def _should_use_flash(self, thinking: dict[str, Any], rag_top_score: float) -> bool:
        if not thinking:
            return True
        if thinking.get("needs_flash"):
            return True
        scores = thinking.get("self_score") or {}
        overall = scores.get("overall")
        if isinstance(overall, float) and overall < 0.75:
            return True  # ìì²´ í‰ê°€ ì ìˆ˜ê°€ ì„ê³„ì¹˜ ë¯¸ë§Œì´ë©´ Flash ìŠ¹ê¸‰
        risk = scores.get("risk")
        if isinstance(risk, float) and risk > 0.6:
            return True
        return False

    def _get_channel_system_prompt(self, channel_id: int | None) -> str:
        """ì±„ë„ë³„ í˜ë¥´ì†Œë‚˜ì™€ ê·œì¹™ì„ ê°€ì ¸ì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        if not channel_id:
            # DMì¸ ê²½ìš° ë¹„ì„œ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©
            return (
                "ë„ˆëŠ” ì‚¬ìš©ìì˜ ê°œì¸ ë¹„ì„œì´ì ì¹œêµ¬ì¸ 'ë§ˆì‚¬ëª½'ì´ì•¼. "
                "í•­ìƒ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” íƒœë„ë¡œ ëŒ€í™”í•´. "
                "ë°˜ë§ê³¼ ì¡´ëŒ“ë§ì„ ì„ì–´ì„œ ì¹œê·¼í•˜ê²Œ ëŒ€í•´ì¤˜."
            )
        channel_config = config.CHANNEL_AI_CONFIG.get(channel_id, {})
        persona = (channel_config.get('persona') or config.DEFAULT_TSUNDERE_PERSONA).strip()
        rules = (channel_config.get('rules') or config.DEFAULT_TSUNDERE_RULES).strip()
        return f"{persona}\n\n{rules}"

    def _compose_main_prompt(
        self,
        message: discord.Message,
        *,
        user_query: str,
        rag_blocks: list[str],
        tool_results_block: str | None,
        fortune_context: str | None = None,
        recent_history: list[dict] | None = None, # [NEW] ìµœê·¼ ëŒ€í™” ê¸°ë¡
    ) -> str:
        """ë©”ì¸ ëª¨ë¸ì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ë¥¼ `emb` ìŠ¤íƒ€ì¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
        
        í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
        1. ì‹œìŠ¤í…œ í˜ë¥´ì†Œë‚˜/ê·œì¹™
        2. [í˜„ì¬ ì‹œê°„] - ì„œë²„ ì‹œê°„ (KST)
        3. [ê³¼ê±° ëŒ€í™” ê¸°ì–µ] - RAG ì»¨í…ìŠ¤íŠ¸
        4. [ë„êµ¬ ì‹¤í–‰ ê²°ê³¼] - ë„êµ¬ ì¶œë ¥ (ìˆì„ ê²½ìš°)
        5. [ì˜¤ëŠ˜ì˜ ìš´ì„¸] - ì‚¬ìš©ì ìš´ì„¸ ì •ë³´ (ìˆì„ ê²½ìš°) [NEW]
        6. [í˜„ì¬ ì§ˆë¬¸] - ì‚¬ìš©ì ì¿¼ë¦¬
        7. ì§€ì‹œì‚¬í•­
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜ + ê·œì¹™)
        system_part = self._get_channel_system_prompt(message.channel.id)

        sections: list[str] = [system_part]

        # ì„œë²„ í˜„ì¬ ì‹œê°„ (KST) - í•­ìƒ í¬í•¨
        current_time = db_utils.get_current_time()
        sections.append(f"[í˜„ì¬ ì‹œê°„]\n{current_time}")

        if fortune_context:
             # [Optimization] ì„¤ëª…ë¬¸ ê°„ì†Œí™”
             sections.append(f"[ìš´ì„¸ ì°¸ê³ ]\n{fortune_context}")

        # [NEW] ë‹¨ê¸° ê¸°ì–µ (ìµœê·¼ ëŒ€í™”) - RAGë³´ë‹¤ ìš°ì„ ìˆœìœ„ ë†’ìŒ
        # [Optimization] ì¤‘ë³µ ì œê±°: ë‹¨ê¸° ê¸°ì–µì— ìˆëŠ” ë‚´ìš©ì€ RAGì—ì„œ ì œê±°í•˜ì—¬ í† í° ì ˆì•½
        recent_context_str = ""
        if recent_history:
            history_text_lines = []
            for item in recent_history:
                role = "User" if item['role'] == 'user' else "Bot"
                text = item['parts'][0] if item['parts'] else ""
                history_text_lines.append(f"{role}: {text}")
            
            if history_text_lines:
                recent_context_str = "\n".join(history_text_lines)
                sections.append(f"[ìµœê·¼ ëŒ€í™” íë¦„ (ë‹¨ê¸° ê¸°ì–µ)]\n{recent_context_str}\n(ìœ„ ëŒ€í™” íë¦„ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ì´ì–´ì§€ëŠ” ë‹µë³€ì„ í•˜ì„¸ìš”.)")

        # RAG ì»¨í…ìŠ¤íŠ¸ (ê³¼ê±° ëŒ€í™” ê¸°ì–µ) - ë‹¨ê¸° ê¸°ì–µê³¼ ì¤‘ë³µë˜ë©´ ì œì™¸
        if rag_blocks:
            filtered_rag = []
            for block in rag_blocks:
                snippet = block[:20] if len(block) > 20 else block
                if snippet not in recent_context_str:
                    # [Optimization] ê° ë¸”ë¡ì„ 500ìë¡œ ì œí•œí•˜ì—¬ í† í° ì ˆì•½
                    truncated_block = block[:500] + "..." if len(block) > 500 else block
                    filtered_rag.append(truncated_block)
            
            if filtered_rag:
                rag_content = "\n\n".join(filtered_rag)
                sections.append(f"[ê³¼ê±° ëŒ€í™” ê¸°ì–µ (ì°¸ê³ ìš©)]\n{rag_content}\n"
                                "(âš ï¸ ì£¼ì˜: ìœ„ ë‚´ìš©ì€ ê³¼ê±°ì˜ ê¸°ì–µì¼ ë¿ì…ë‹ˆë‹¤. í˜„ì¬ ëŒ€í™”ê°€ ì•„ë‹™ë‹ˆë‹¤. "
                                "ì‚¬ìš©ìê°€ ê³¼ê±°ì— ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ í–ˆë”ë¼ë„, 'ì•„ê¹Œ ë§í–ˆì–ì•„'ë¼ê³  í•˜ì§€ ë§ê³  "
                                "ë§ˆì¹˜ ì²˜ìŒ ë“£ëŠ” ê²ƒì²˜ëŸ¼ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.)")

        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ - ëˆ„ë½ ë³µêµ¬
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ - ëˆ„ë½ ë³µêµ¬
        if tool_results_block:
            sections.append(f"[ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ (ìµœìš°ì„  ì •ë³´)]\n{tool_results_block}")
            sections.append("(âš ï¸ ì ˆëŒ€ì  ì§€ì¹¨: ìœ„ [ë„êµ¬ ì‹¤í–‰ ê²°ê³¼]ëŠ” ë°©ê¸ˆ ì¡°íšŒí•œ **ì‹¤ì‹œê°„ ì‚¬ì‹¤**ì…ë‹ˆë‹¤. \n"
                            "1. ê²°ê³¼ì— ë°ì´í„°(ì£¼ê°€, ë‚ ì”¨ ë“±)ê°€ ìˆë‹¤ë©´, **ë¬´ì¡°ê±´** ì´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´.\n"
                            "2. 'ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆë‹¤'ê³  ê±°ì§“ë§í•˜ì§€ ë§ˆ.\n"
                            "3. ë§Œì•½ ê²°ê³¼ì— 'Error'ë‚˜ 'ì‹¤íŒ¨'ë¼ê³  ì í˜€ìˆë‹¤ë©´, ê·¸ë•Œë§Œ ì‹¤íŒ¨í–ˆë‹¤ê³  ë§í•´.\n"
                            "4. ì£¼ê°€ ì •ë³´ì˜ ê²½ìš°, 'í˜„ì¬ê°€', 'ë“±ë½ìœ¨', 'ì‹œê°€ì´ì•¡'ì„ ê¼­ ì–¸ê¸‰í•´ì¤˜.)")


        # í˜„ì¬ ì§ˆë¬¸
        sections.append(f"[í˜„ì¬ ì§ˆë¬¸]\n{user_query}")

        # ì§€ì‹œì‚¬í•­ - RAG ë°ì´í„°ë¥¼ ë°°ê²½ ì§€ì‹ìœ¼ë¡œ ì·¨ê¸‰í•˜ë„ë¡ ëª…ì‹œ
        if rag_blocks:
            sections.append(
                "ìœ„ ê¸°ì–µì€ ê³¼ê±° ëŒ€í™”ì—ì„œ ê°€ì ¸ì˜¨ ë°°ê²½ ì •ë³´ì•¼. "
                "'ì•„ê¹Œ', 'ì „ì—', 'ë°©ê¸ˆ' ê°™ì€ ì‹œê°„ í‘œí˜„ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´. "
                "ê°™ì€ ì£¼ì œë¼ë„ ì²˜ìŒ ë“£ëŠ” ê²ƒì²˜ëŸ¼ ìƒˆë¡­ê²Œ, ë„¤ ì£¼ê´€ê³¼ ê°ì •ì„ ì„ì–´ì„œ ëŒ€ë‹µí•´ì¤˜."
            )
        else:
            sections.append("ê´€ë ¨ ê¸°ì–µì€ ì—†ì§€ë§Œ, ë„ˆë§Œì˜ ì£¼ê´€ì ì¸ ì˜ê²¬ì´ë‚˜ ë¦¬ì•¡ì…˜ì„ ì„ì–´ì„œ ì™„ì „í•œ ì¹œêµ¬ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì¤˜.")

        return "\n\n".join(sections)

    def _parse_tool_calls(self, text: str) -> list[dict]:
        """Lite ëª¨ë¸ì˜ ì‘ë‹µì—ì„œ <tool_plan> ë˜ëŠ” <tool_call> XML íƒœê·¸ë¥¼ íŒŒì‹±í•˜ì—¬ JSONìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        plan_match = re.search(r'<tool_plan>\s*(\[.*?\])\s*</tool_plan>', text, re.DOTALL)
        if plan_match:
            try:
                calls = json.loads(plan_match.group(1))
                if isinstance(calls, list):
                    logger.info(f"ë„êµ¬ ê³„íš(plan)ì„ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤: {len(calls)} ë‹¨ê³„")
                    return calls
            except json.JSONDecodeError as e:
                logger.warning(f"tool_plan JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}. ì›ë³¸: {plan_match.group(1)}")
                return []

        call_match = re.search(r'<tool_call>\s*(\{.*?\})\s*</tool_call>', text, re.DOTALL)
        if call_match:
            try:
                call = json.loads(call_match.group(1))
                if isinstance(call, dict):
                    logger.info("ë‹¨ì¼ ë„êµ¬ í˜¸ì¶œ(call)ì„ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.")
                    return [call]
            except json.JSONDecodeError as e:
                logger.warning(f"tool_call JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}. ì›ë³¸: {call_match.group(1)}")

        return []

    @staticmethod
    def _format_tool_results_for_prompt(tool_results: list[dict]) -> str:
        lines: list[str] = []
        for entry in tool_results:
            name = entry.get("tool_name") or "unknown"
            result = entry.get("result") or {}

            # [Optimization] RAG ê²°ê³¼ í¬ë§·íŒ… (ê¸°ì¡´ ìœ ì§€ í™•ì¸)
            if name == "local_rag":
                # ... (RAG ì²˜ë¦¬ëŠ” ìœ„ ë©”ì„œë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ì—ˆì–´ì•¼ í•¨, ì•„ë˜ ë®ì–´ì“°ë¯€ë¡œ ì£¼ì˜)
                # ì—¬ê¸°ì„œëŠ” RAGë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë„êµ¬ë§Œ ìµœì í™”í•˜ê³  RAGëŠ” ê¸°ì¡´ ë¡œì§ì„ ê°€ì ¸ì™€ì•¼ í•¨.
                # í¸ì˜ìƒ RAG ë¡œì§ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , ì¼ë°˜ ë„êµ¬ í¬ë§·íŒ…ë§Œ ê°œì„ 
                entries = []
                if isinstance(result, dict):
                    raw_entries = result.get("entries")
                    if isinstance(raw_entries, list):
                        entries = [item for item in raw_entries if isinstance(item, dict)]
                if entries:
                    for idx, rag_entry in enumerate(entries, start=1):
                        block = (rag_entry.get("dialogue_block") or rag_entry.get("message") or "").strip()
                        if not block: continue
                        score = rag_entry.get("combined_score")
                        header = f"[local_rag #{idx}]"
                        if isinstance(score, (int, float)):
                            header += f" score={float(score):.3f}"
                        lines.append(header)
                        for line in block.splitlines():
                            lines.append(f"  {line}")
                continue

            # [Optimization] ë‚ ì”¨ ë„êµ¬ ê²°ê³¼ ìµœì í™”
            if name == "get_weather_forecast" and isinstance(result, dict):
                # 1. Location & Current Weather
                location = result.get("location", "")
                current = result.get("current_weather", "")
                if location or current:
                    lines.append(f"[{name}] {location} í˜„ì¬ ë‚ ì”¨: {current}")

                # 2. Short-term Forecast Items
                items = result.get("forecast_items") or result.get("items", [])
                if items:
                    formatted_wx = []
                    for item in items[:5]: # 5ê°œ ì˜ˆë³´ë§Œ ì‚¬ìš© (ê°€ì¥ ê°€ê¹Œìš´ ë¯¸ë˜)
                        time_str = item.get("fcstTime", "")
                        temp = item.get("TMP", "?")
                        sky = item.get("SKY", "?") 
                        rain = item.get("POP", "?")
                        formatted_wx.append(f"{time_str}ì‹œ: {temp}ë„, ê°•ìˆ˜{rain}%, {sky}")
                    
                    result_text = " | ".join(formatted_wx)
                    lines.append(f"[{name}] ë‹¨ê¸° ì˜ˆë³´: {result_text}")
                elif not current:
                    # Fallback if both empty but dict exists (legacy or error?)
                    lines.append(f"[{name}] {str(result)}")
                continue

            # [Optimization] ì£¼ì‹ ë„êµ¬ ê²°ê³¼ ìµœì í™”
            # [Optimization] ì£¼ì‹ ë„êµ¬ ê²°ê³¼ ìµœì í™”
            if name == "get_stock_price":
                # yfinance ëª¨ë“œëŠ” ì´ë¯¸ í¬ë§·ëœ Markdown ë¬¸ìì—´ì„ ë°˜í™˜í•¨
                if isinstance(result, str):
                    # ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥ (íŠ¸ë ì¼€ì´ì…˜ ì—†ì´ ì¤‘ìš” ì •ë³´ ë³´ì¡´)
                    lines.append(f"[{name}] (ê²°ê³¼ ë°ì´í„°)\n{result}")
                    continue
                elif isinstance(result, dict):
                     # Legacy (Finnhub/KRX) dict return
                    curr = result.get("c" if "c" in result else "ItemPrice", "?") 
                    change = result.get("d" if "d" in result else "FluctuationRate", "?")
                    lines.append(f"[{name}] í˜„ì¬ê°€: {curr}, ë“±ë½: {change}")
                    continue
            
            # [Optimization] ë‚˜ë¨¸ì§€ ë„êµ¬ëŠ” ë¬¸ìì—´ ê¸¸ì´ ì œí•œ
            if isinstance(result, dict):
                result_text = json.dumps(result, ensure_ascii=False)
            else:
                result_text = str(result)
            
            # 500ì ì´ìƒì´ë©´ ìë¦„
            if len(result_text) > 500:
                result_text = result_text[:500] + "...(ìƒëµ)"
            
            lines.append(f"[{name}] {result_text}")

        return "\n".join(lines) if lines else "ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ì—†ìŒ"

    async def _send_split_message(self, message: discord.Message, text: str):
        """
        2000ìê°€ ë„˜ëŠ” ë©”ì‹œì§€ë¥¼ ì•ˆì „í•˜ê²Œ ë‚˜ëˆ„ì–´ ì „ì†¡í•©ë‹ˆë‹¤.
        Discordì˜ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ(2000ì)ì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤.
        """
        if not text:
            return

        # 1900ìë¡œ ì—¬ìœ  ìˆê²Œ ì„¤ì • (ê¸°íƒ€ í¬ë§·íŒ… ê³ ë ¤)
        chunk_size = 1900
        
        # í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ë°”ë¡œ ì „ì†¡
        if len(text) <= chunk_size:
            await message.reply(text, mention_author=False)
            return

        # ê¸´ í…ìŠ¤íŠ¸ ë¶„í•  ì „ì†¡
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        
        for i, chunk in enumerate(chunks):
            # ì²« ë²ˆì§¸ ë©”ì‹œì§€ëŠ” replyë¡œ, ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜ ë©”ì‹œì§€ë¡œ ì „ì†¡í•˜ì—¬ ìŠ¤ë ˆë“œì²˜ëŸ¼ ë³´ì´ê²Œ í•¨
            if i == 0:
                await message.reply(chunk, mention_author=False)
            else:
                await message.channel.send(chunk)
            # ìˆœì„œ ë³´ì¥ì„ ìœ„í•œ ì§§ì€ í…€
            await asyncio.sleep(0.5)

    @staticmethod
    def _build_rag_debug_block(entries: list[dict]) -> str:
        """RAG í›„ë³´ë¥¼ ë¡œê·¸ë¡œ ë‚¨ê¸°ê¸° ìœ„í•œ í¬ë§·í„°."""
        if not config.RAG_DEBUG_ENABLED or not entries:
            return ""

        lines: list[str] = []
        for entry in entries:
            block = entry.get("dialogue_block") or entry.get("message") or ""
            snippet = block if len(block) <= 200 else block[:197] + "..."
            origin = entry.get("origin") or "?"
            score = entry.get("combined_score") or 0.0
            lines.append(f"origin={origin} | score={float(score):.3f} | {snippet}")

        return "```debug\n" + "\n".join(lines) + "\n```"

    async def _execute_tool(self, tool_call: dict, guild_id: int, user_query: str) -> dict:
        """íŒŒì‹±ëœ ë‹¨ì¼ ë„êµ¬ í˜¸ì¶œ ê³„íšì„ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        tool_name = tool_call.get('tool_to_use') or tool_call.get('tool_name')
        if tool_name and 'tool_to_use' not in tool_call:
            tool_call['tool_to_use'] = tool_name
        parameters = tool_call.get('parameters', {})
        log_extra = {'guild_id': guild_id, 'tool_name': tool_name, 'parameters': parameters}

        if not tool_name: 
            return {"error": "tool_to_useê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        # web_searchëŠ” Google Custom Search APIì™€ LLM 2-step ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if tool_name == 'web_search':
            logger.info("íŠ¹ë³„ ë„êµ¬ ì‹¤í–‰: web_search (Google Custom Search API)", extra=log_extra)
            query = parameters.get('query', user_query)
            self._debug(f"[ë„êµ¬:web_search] ì¿¼ë¦¬: {self._truncate_for_debug(query)}", log_extra)
            
            # ì¼ì¼ ì œí•œ í™•ì¸
            if await self._check_daily_search_limit():
                return {"error": "Google Custom Search API ì¼ì¼ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤."}
            
            search_result = await self._execute_web_search_with_llm(query, log_extra)
            if search_result.get("result"):
                self._debug(f"[ë„êµ¬:web_search] ê²°ê³¼: {self._truncate_for_debug(search_result)}", log_extra)
                return search_result
            return {"error": search_result.get("error", "ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì •ë³´ë¥¼ ì°¾ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")}

        # generate_imageëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„± + CometAPI í˜¸ì¶œ 2-step ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if tool_name == 'generate_image':
            logger.info("íŠ¹ë³„ ë„êµ¬ ì‹¤í–‰: generate_image (CometAPI flux-2-flex)", extra=log_extra)
            original_query = parameters.get('user_query', user_query)
            user_id = parameters.get('user_id')
            
            if user_id is None:
                return {"error": "ì´ë¯¸ì§€ ìƒì„±ì— í•„ìš”í•œ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}
            
            # LLMì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ìµœì í™”
            image_prompt = await self._generate_image_prompt(original_query, log_extra)
            if not image_prompt:
                return {"error": "ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì¤˜!"}
            
            self._debug(f"[ë„êµ¬:generate_image] ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸: {self._truncate_for_debug(image_prompt)}", log_extra)
            
            # ToolsCogì˜ generate_image ë„êµ¬ í˜¸ì¶œ
            result = await self.tools_cog.generate_image(prompt=image_prompt, user_id=user_id)
            return result

        # ê·¸ ì™¸ ì¼ë°˜ ë„êµ¬ë“¤ì€ ToolsCogì—ì„œ ì°¾ì•„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        try:
            tool_method = getattr(self.tools_cog, tool_name)
            logger.info(f"ì¼ë°˜ ë„êµ¬ ì‹¤í–‰: {tool_name} with params: {parameters}", extra=log_extra)
            self._debug(f"[ë„êµ¬:{tool_name}] íŒŒë¼ë¯¸í„°: {self._truncate_for_debug(parameters)}", log_extra)
            result = await tool_method(**parameters)
            self._debug(f"[ë„êµ¬:{tool_name}] ê²°ê³¼: {self._truncate_for_debug(result)}", log_extra)
            if not isinstance(result, dict):
                return {"result": str(result)}
            return result
        except AttributeError:
            logger.error(f"ë„êµ¬ '{tool_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", extra=log_extra)
            return {"error": f"'{tool_name}'ì´ë¼ëŠ” ë„êµ¬ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
        except Exception as e:
            logger.error(f"ë„êµ¬ '{tool_name}' ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}", exc_info=True, extra=log_extra)
            return {"error": "ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}


    async def process_agent_message(self, message: discord.Message):
        """2-Step Agentì˜ ì „ì²´ íë¦„ì„ ê´€ë¦¬í•©ë‹ˆë‹¤."""
        if not self.is_ready:
            return

        base_log_extra = {
            'guild_id': message.guild.id if message.guild else None,
            'channel_id': message.channel.id,
            'user_id': message.author.id,
        }
        
        # ========== ì•ˆì „ì¥ì¹˜ ê²€ì‚¬ ==========
        user_id = message.author.id
        now = datetime.now()
        
        # 1. ì‚¬ìš©ìë³„ ì¿¨ë‹¤ìš´ ê²€ì‚¬
        last_request = self.ai_user_cooldowns.get(user_id)
        if last_request:
            elapsed = (now - last_request).total_seconds()
            if elapsed < config.USER_COOLDOWN_SECONDS:
                remaining = config.USER_COOLDOWN_SECONDS - elapsed
                logger.debug(f"ì‚¬ìš©ì {user_id} ì¿¨ë‹¤ìš´ ì¤‘ ({remaining:.1f}ì´ˆ ë‚¨ìŒ)", extra=base_log_extra)
                return
        
        # 2. ìŠ¤íŒ¸ ë°©ì§€: ë™ì¼ ë©”ì‹œì§€ ë°˜ë³µ ê°ì§€
        user_msg_key = f"{user_id}:{message.content[:50]}"
        spam_cache = getattr(self, '_spam_cache', {})
        if user_msg_key in spam_cache:
            if (now - spam_cache[user_msg_key]).total_seconds() < config.SPAM_PREVENTION_SECONDS:
                logger.warning(f"ìŠ¤íŒ¸ ê°ì§€: ì‚¬ìš©ì {user_id}ê°€ ë™ì¼ ë©”ì‹œì§€ ë°˜ë³µ", extra=base_log_extra)
                return
        
        # [Safety] DM Loop Prevention: Detect rapid self-responses or bot-loops
        if not message.guild:
             # Check if the channel has very recent messages from THIS bot
             async for hist_msg in message.channel.history(limit=5):
                 if hist_msg.author.id == self.bot.user.id:
                     if (now.replace(tzinfo=timezone.utc) - hist_msg.created_at.replace(tzinfo=timezone.utc)).total_seconds() < 2.0:
                         logger.warning("DM Loop Detected: Bot replied too recently.", extra=base_log_extra)
                         return
                     break # Only check the most recent bot message

        spam_cache[user_msg_key] = now
        # ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬ (100ê°œ ì´ˆê³¼ ì‹œ)
        if len(spam_cache) > 100:
            oldest_keys = sorted(spam_cache.keys(), key=lambda k: spam_cache[k])[:50]
            for k in oldest_keys:
                del spam_cache[k]
        self._spam_cache = spam_cache
        
        # 3. ì‚¬ìš©ìë³„ ì¼ì¼ LLM í˜¸ì¶œ ì œí•œ ê²€ì‚¬
        user_daily_key = f"llm_user_{user_id}"
        user_daily_count = await db_utils.get_daily_api_count(self.bot.db, user_daily_key)
        if user_daily_count >= config.USER_DAILY_LLM_LIMIT:
            logger.warning(f"ì‚¬ìš©ì {user_id} ì¼ì¼ LLM ì œí•œ ë„ë‹¬ ({user_daily_count}/{config.USER_DAILY_LLM_LIMIT})", extra=base_log_extra)
            await message.reply("ì˜¤ëŠ˜ ë„ˆë¬´ ë§ì´ ë¬¼ì–´ë´¤ì–´! ë‚´ì¼ ë‹¤ì‹œ ë¬¼ì–´ë´~ ğŸ˜…", mention_author=False)
            return
        
        # 4. ê¸€ë¡œë²Œ ì¼ì¼ LLM í˜¸ì¶œ ì œí•œ ê²€ì‚¬
        global_daily_count = await db_utils.get_daily_api_count(self.bot.db, "llm_global")
        if global_daily_count >= config.GLOBAL_DAILY_LLM_LIMIT:
            logger.warning(f"ê¸€ë¡œë²Œ ì¼ì¼ LLM ì œí•œ ë„ë‹¬ ({global_daily_count}/{config.GLOBAL_DAILY_LLM_LIMIT})", extra=base_log_extra)
            await message.reply("ì˜¤ëŠ˜ í•  ìˆ˜ ìˆëŠ” ëŒ€í™”ê°€ ë‹¤ ëë‚¬ì–´... ë‚´ì¼ ë´! ğŸ˜¢", mention_author=False)
            return
        
        # ì¿¨ë‹¤ìš´ ê°±ì‹ 
        self.ai_user_cooldowns[user_id] = now
        # ========== ì•ˆì „ì¥ì¹˜ ê²€ì‚¬ ì™„ë£Œ ==========
        
        user_query = self._prepare_user_query(message, base_log_extra)
        if not user_query:
            return

        # 5. DM Rate Limiting Check (New)
        if not message.guild:
            # 5-1. ì‚¬ìš©ìë³„ 1:1 ì œí•œ (3ì‹œê°„ 5íšŒ)
            allowed, reset_time = await db_utils.check_dm_message_limit(self.bot.db, user_id)
            if not allowed:
                 await message.reply(
                     f"â›” ì¼ì¼ ëŒ€í™”ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\në§ˆì‚¬ëª½ê³¼ì˜ 1:1 ëŒ€í™”ëŠ” 5ì‹œê°„ë‹¹ 30íšŒë¡œ ì œí•œë©ë‹ˆë‹¤.\nğŸ•’ í•´ì œ ì˜ˆì • ì‹œê°: {reset_time}",
                     mention_author=False
                 )
                 return
            
            # 5-2. ì „ì—­ ì¼ì¼ DM ì œí•œ (í•˜ë£¨ 100íšŒ - API ë³´í˜¸)
            if not await db_utils.check_global_dm_limit(self.bot.db):
                await message.reply(
                    "â›” ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ ë§ˆì‚¬ëª½ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” DM ì´ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\në‚´ì¼ ë‹¤ì‹œ ì´ìš©í•´ ì£¼ì„¸ìš”! (ì„œë²„ ì±„ë„ì—ì„œëŠ” ê³„ì† ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤)",
                    mention_author=False
                )
                return

        trace_id = uuid.uuid4().hex[:8]
        log_extra = dict(base_log_extra)
        log_extra['trace_id'] = trace_id
        logger.info(f"ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì‹œì‘. Query: '{user_query}'", extra=log_extra)
        self._debug(f"--- ì—ì´ì „íŠ¸ ì„¸ì…˜ ì‹œì‘ trace_id={trace_id}", log_extra)

        async with message.channel.typing():
            try:
                # [NEW] ì§€ì—­ëª… ìºì‹œ ë¡œë“œ (í•„ìš” ì‹œ)
                await self._load_location_cache()

                recent_search_messages = await self._collect_recent_search_messages(message)
                guild_id_safe = message.guild.id if message.guild else 0
                rag_prompt, rag_entries, rag_top_score, rag_blocks = await self._get_rag_context(
                    guild_id_safe,
                    message.channel.id,
                    message.author.id,
                    user_query,
                    recent_messages=recent_search_messages,
                )
                history = await self._get_recent_history(message, rag_prompt)
                rag_is_strong = bool(rag_blocks) and rag_top_score >= config.RAG_STRONG_SIMILARITY_THRESHOLD
                self._debug(
                    f"RAG ê²°ê³¼: strong={rag_is_strong} top_score={rag_top_score:.3f} blocks={len(rag_blocks)}",
                    log_extra,
                )

                # ========== ë‹¨ì¼ ëª¨ë¸ ì•„í‚¤í…ì²˜: Lite ëª¨ë¸ ì œê±°, í‚¤ì›Œë“œ ê¸°ë°˜ ë„êµ¬ ê°ì§€ ==========
                # í‚¤ì›Œë“œ íŒ¨í„´ìœ¼ë¡œ ë„êµ¬ í•„ìš” ì—¬ë¶€ íŒë‹¨ (API í˜¸ì¶œ ì—†ìŒ)
                tool_plan = self._detect_tools_by_keyword(user_query)
                if tool_plan:
                    logger.info(f"í‚¤ì›Œë“œ ê¸°ë°˜ ë„êµ¬ ê°ì§€: {[t['tool_to_use'] for t in tool_plan]}", extra=log_extra)
                else:
                    logger.info("ë„êµ¬ í•„ìš” ì—†ìŒ - RAG/ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬", extra=log_extra)

                tool_results: list[dict[str, Any]] = []
                executed_plan: list[dict[str, Any]] = []

                if rag_blocks:
                    tool_results.append(
                        {
                            "step": 0,
                            "tool_name": "local_rag",
                            "parameters": {"top_score": rag_top_score},
                            "result": {"entries": rag_entries},
                        }
                    )

                if tool_plan:
                    logger.info(f"2ë‹¨ê³„: ë„êµ¬ ì‹¤í–‰ ì‹œì‘. ì´ {len(tool_plan)}ë‹¨ê³„.", extra=log_extra)
                    self._debug(f"ë„êµ¬ ê³„íš: {self._truncate_for_debug(tool_plan)}", log_extra)
                    for idx, tool_call in enumerate(tool_plan, start=1):
                        logger.info(f"ê³„íš ì‹¤í–‰ ({idx}/{len(tool_plan)}): {tool_call.get('tool_to_use')}", extra=log_extra)
                        
                        # generate_image ë„êµ¬ì˜ ê²½ìš° user_idë¥¼ íŒŒë¼ë¯¸í„°ì— ì£¼ì…
                        if tool_call.get('tool_to_use') == 'generate_image':
                            tool_call.setdefault('parameters', {})['user_id'] = message.author.id
                            # ìƒì„± ì¤‘ ë©”ì‹œì§€ ì „ì†¡ (LLM í˜¸ì¶œ ì—†ìŒ)
                            status_msg = await message.reply("ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘ì´ì—ìš”... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì¤˜!", mention_author=False)
                        
                        result = await self._execute_tool(tool_call, guild_id_safe, user_query)
                        
                        # ì´ë¯¸ì§€ ìƒì„± ì„±ê³µ ì‹œ ë°”ë¡œ ì´ë¯¸ì§€ ì „ì†¡ (ë³„ë„ ì²˜ë¦¬)
                        if tool_call.get('tool_to_use') == 'generate_image' and (result.get('image_data') or result.get('image_url')):
                            remaining = result.get('remaining', 0)
                            logger.info(f"ì´ë¯¸ì§€ ìƒì„± ì„±ê³µ, ì „ì†¡ ì‹œì‘", extra=log_extra)
                            
                            # ìƒíƒœ ë©”ì‹œì§€ ì‚­ì œ
                            try:
                                await status_msg.delete()
                            except:
                                pass
                            
                            # ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ê°€ ìˆìœ¼ë©´ íŒŒì¼ë¡œ ì—…ë¡œë“œ (URL ë§Œë£Œ ë°©ì§€)
                            if result.get('image_data'):
                                import io
                                image_file = discord.File(
                                    io.BytesIO(result['image_data']),
                                    filename="generated_image.jpg"
                                )
                                await message.reply(
                                    f"ì§œì”~ ì´ë¯¸ì§€ ìƒì„±í–ˆì–´! ğŸ¨\n(ë‚¨ì€ ì´ë¯¸ì§€ ìƒì„± íšŸìˆ˜: {remaining}ì¥)",
                                    file=image_file,
                                    mention_author=False
                                )
                            else:
                                # í´ë°±: URLë¡œ ì „ì†¡
                                await message.reply(
                                    f"ì§œì”~ ì´ë¯¸ì§€ ìƒì„±í–ˆì–´! ğŸ¨\n{result['image_url']}\n\n(ë‚¨ì€ ì´ë¯¸ì§€ ìƒì„± íšŸìˆ˜: {remaining}ì¥)",
                                    mention_author=False
                                )
                            
                            # LLM í˜¸ì¶œ ì¹´ìš´í„° ì¦ê°€
                            await db_utils.log_api_call(self.bot.db, f"llm_user_{message.author.id}")
                            await db_utils.log_api_call(self.bot.db, "llm_global")
                            
                            await db_utils.log_analytics(
                                self.bot.db,
                                "AI_INTERACTION",
                                {
                                    "guild_id": guild_id_safe,
                                    "user_id": message.author.id,
                                    "channel_id": message.channel.id,
                                    "trace_id": trace_id,
                                    "mode": "image_generation",
                                },
                            )
                            return  # ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ, ì¶”ê°€ ì²˜ë¦¬ ì—†ì´ ì¢…ë£Œ
                        
                        # ì´ë¯¸ì§€ ìƒì„± ì—ëŸ¬ ì‹œ ìƒíƒœ ë©”ì‹œì§€ ìˆ˜ì •
                        if tool_call.get('tool_to_use') == 'generate_image' and result.get('error'):
                            error_msg = result['error']
                            logger.warning(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {error_msg}", extra=log_extra)
                            try:
                                await status_msg.edit(content=f"ğŸ˜… {error_msg}")
                            except:
                                await message.reply(f"ğŸ˜… {error_msg}", mention_author=False)
                            return  # ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨, ì¶”ê°€ ì²˜ë¦¬ ì—†ì´ ì¢…ë£Œ
                        
                        tool_results.append(
                            {
                                "step": idx,
                                "tool_name": tool_call.get('tool_to_use'),
                                "parameters": tool_call.get('parameters'),
                                "result": result,
                            }
                        )
                        executed_plan.append(tool_call)
                else:
                    # ë„êµ¬ ê³„íšì´ ì—†ì„ ë•Œ, ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€ ìë™ íŒë‹¨
                    if await self._should_use_web_search(user_query, rag_top_score):
                        logger.info("ìë™ íŒë‹¨: ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë¨", extra=log_extra)
                        web_result = await self._execute_web_search_with_llm(user_query, log_extra)
                        
                        # ì›¹ ê²€ìƒ‰ ìš”ì•½ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì‘ë‹µ (3ë²ˆì§¸ LLM í˜¸ì¶œ ë°©ì§€)
                        if web_result.get("summary"):
                            final_response_text = web_result["summary"]
                            logger.info("ì›¹ ê²€ìƒ‰ ìš”ì•½ì„ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©", extra=log_extra)
                            
                            # LLM ì¼ì¼ ì¹´ìš´í„° ì¦ê°€ (ì•ˆì „ì¥ì¹˜)
                            await db_utils.log_api_call(self.bot.db, f"llm_user_{message.author.id}")
                            await db_utils.log_api_call(self.bot.db, "llm_global")
                            
                            await message.reply(final_response_text, mention_author=False)
                            await db_utils.log_analytics(
                                self.bot.db,
                                "AI_INTERACTION",
                                {
                                    "guild_id": guild_id_safe,
                                    "user_id": message.author.id,
                                    "channel_id": message.channel.id,
                                    "trace_id": trace_id,
                                    "mode": "web_search_auto",
                                },
                            )
                            return  # ì—¬ê¸°ì„œ ì¢…ë£Œ - ì¶”ê°€ LLM í˜¸ì¶œ ë°©ì§€
                        
                        # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ í´ë°±
                        if web_result.get("result"):
                            tool_results.append(
                                {
                                    "step": 1,
                                    "tool_name": "web_search",
                                    "parameters": {"query": user_query, "auto_triggered": True},
                                    "result": web_result,
                                }
                            )
                            executed_plan.append({"tool_to_use": "web_search", "parameters": {"query": user_query}})
                    else:
                        logger.info("ë„êµ¬ ê³„íš ì—†ìŒ - RAG/ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬", extra=log_extra)

                executed_tool_results = [res for res in tool_results if res.get("tool_name") not in {"local_rag"}]

                def _is_tool_failed(result_obj: Any) -> bool:
                    if result_obj is None:
                        return True
                    lowered = str(result_obj).lower()
                    failure_keywords = ["error", "ì˜¤ë¥˜", "ì‹¤íŒ¨", "ì—†ìŠµë‹ˆë‹¤", "ì•Œ ìˆ˜ ì—†ëŠ”", "ì°¾ì„ ìˆ˜"]
                    return any(keyword in lowered for keyword in failure_keywords)

                any_failed = any(_is_tool_failed(res.get("result")) for res in executed_tool_results)
                executed_tool_names = {res.get("tool_name") for res in executed_tool_results}
                use_fallback_prompt = False

                if executed_tool_results and any_failed and 'web_search' not in executed_tool_names:
                    logger.info("í•˜ë‚˜ ì´ìƒì˜ ë„êµ¬ ì‹¤í–‰ì— ì‹¤íŒ¨í•˜ì—¬ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.", extra=log_extra)
                    web_result = await self._execute_tool(
                        {"tool_to_use": "web_search", "parameters": {"query": user_query}},
                        guild_id_safe,
                        user_query,
                    )
                    tool_results = [res for res in tool_results if res.get("tool_name") == "local_rag"]
                    tool_results.append(
                        {
                            "step": len(tool_results) + 1,
                            "tool_name": "web_search",
                            "parameters": {"query": user_query},
                            "result": web_result,
                        }
                    )
                    use_fallback_prompt = True

                tool_results_str = self._format_tool_results_for_prompt(tool_results)
                if len(tool_results_str) > 3800:
                    tool_results_str = tool_results_str[:3800]  # Gemini ì…ë ¥ ì œí•œ ë³´í˜¸


                # ë‹¨ì¼ ëª¨ë¸ ì•„í‚¤í…ì²˜: Main ëª¨ë¸ í˜¸ì¶œ
                system_prompt = config.WEB_FALLBACK_PROMPT if use_fallback_prompt else config.AGENT_SYSTEM_PROMPT
                rag_blocks_for_prompt = [] if use_fallback_prompt else rag_blocks
                
                # [NEW] ìš´ì„¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ (DMì¸ ê²½ìš°ì—ë§Œ)
                fortune_context = None
                if not message.guild and self.bot.db:
                    try:
                        # ì˜¤ëŠ˜ ë‚ ì§œ í™•ì¸
                        today_str = datetime.now(pytz.timezone('Asia/Seoul')).strftime('%Y-%m-%d')
                        # êµ¬ë… ë°œì†¡ ê¸°ë¡(last_fortune_sent)ì€ YYYY-MM-DD
                        # last_fortune_contentê°€ ì–¸ì œ ì €ì¥ë˜ì—ˆëŠ”ì§€ ë³„ë„ ì»¬ëŸ¼ì´ ì—†ì§€ë§Œ,
                        # last_fortune_sentê°€ 'ì˜¤ëŠ˜'ì´ë©´ last_fortune_contentë„ 'ì˜¤ëŠ˜'ê²ƒì¼ í™•ë¥ ì´ ë†’ìŒ.
                        # ë‹¤ë§Œ sentê°€ ì—…ë°ì´íŠ¸ ì•ˆë˜ê³  contentë§Œ ì—…ë°ì´íŠ¸(ì§ì ‘ì¡°íšŒ) ë  ìˆ˜ ìˆìŒ.
                        # ì—¬ê¸°ì„œëŠ” last_fortune_contentê°€ nullì´ ì•„ë‹ˆë©´ ì¼ë‹¨ ê°€ì ¸ì˜¤ë˜,
                        # ë‚´ìš© ì•ˆì— ë‚ ì§œê°€ ì—†ë‹¤ë©´... ìŒ.
                        # ì¼ë‹¨ ë‹¨ìˆœíˆ ê°€ì ¸ì™€ë³´ì. (user_profilesì— last_gen_dateê°€ ìˆìœ¼ë©´ ì¢‹ê² ì§€ë§Œ sentë¥¼ í™œìš©í•˜ê±°ë‚˜)
                        # ì—¬ê¸°ì„œëŠ” contentë§Œ ê°€ì ¸ì˜´.
                        row = await self.bot.db.execute("SELECT last_fortune_content FROM user_profiles WHERE user_id = ?", (message.author.id,)) # 
                        res = await row.fetchone()
                        if res and res[0]:
                             fortune_context = res[0]
                    except Exception as e:
                        logger.error(f"ìš´ì„¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")

                main_prompt = self._compose_main_prompt(
                    message,
                    user_query=user_query,
                    rag_blocks=rag_blocks_for_prompt,
                    tool_results_block=tool_results_str if tool_results_str else None,
                    fortune_context=fortune_context,
                    recent_history=history, # [NEW] íˆìŠ¤í† ë¦¬ ì£¼ì…
                )

                final_response_text = ""

                # CometAPI ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ Geminië¡œ í´ë°±
                if self.use_cometapi:
                    logger.info("CometAPI(ë‹µë³€ ìƒì„±) í˜¸ì¶œ...", extra=log_extra)
                    final_response_text = await self._cometapi_generate_content(
                        system_prompt, main_prompt, log_extra
                    ) or ""
                
                # CometAPI ì‹¤íŒ¨ ë˜ëŠ” ë¹„í™œì„±í™” ì‹œ Gemini ì‚¬ìš©
                if not final_response_text and self.gemini_configured and genai:
                    logger.info("Gemini(ë‹µë³€ ìƒì„±) í˜¸ì¶œ...", extra=log_extra)
                    main_model = genai.GenerativeModel(
                        config.AI_RESPONSE_MODEL_NAME,
                        system_instruction=system_prompt,
                    )
                    self._debug(f"[Gemini] system_prompt={self._truncate_for_debug(system_prompt)}", log_extra)
                    self._debug(f"[Gemini] user_prompt={self._truncate_for_debug(main_prompt)}", log_extra)
                    main_response = await self._safe_generate_content(main_model, main_prompt, log_extra)
                    if main_response and main_response.parts:
                        try:
                            final_response_text = main_response.text.strip()
                        except ValueError:
                            pass
                
                if final_response_text:
                    self._debug(f"[Main] ìµœì¢… ì‘ë‹µ: {self._truncate_for_debug(final_response_text)}", log_extra)
                    debug_block = self._build_rag_debug_block(rag_entries)
                    if debug_block:
                        logger.debug("RAG ë””ë²„ê·¸ ë¸”ë¡:\n%s", debug_block, extra=log_extra)
                    
                    # LLM ì¼ì¼ ì¹´ìš´í„° ì¦ê°€ (ì•ˆì „ì¥ì¹˜)
                    await db_utils.log_api_call(self.bot.db, f"llm_user_{message.author.id}")
                    await db_utils.log_api_call(self.bot.db, "llm_global")

                    # ì‘ë‹µ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬: ìê¸° ìì‹  ë©˜ì…˜(@ë§ˆì‚¬ëª½ ë“±) ì œê±°
                    final_response_text = re.sub(r'^@ë§ˆì‚¬ëª½\s*', '', final_response_text)
                    final_response_text = re.sub(r'^@masamong\s*', '', final_response_text, flags=re.IGNORECASE)
                    final_response_text = re.sub(r'^<@!?[0-9]+>\s*', '', final_response_text)
                    
                    await self._send_split_message(message, final_response_text)
                    await db_utils.log_analytics(
                        self.bot.db,
                        "AI_INTERACTION",
                        {
                            "guild_id": message.guild.id if message.guild else "DM",
                            "user_id": message.author.id,
                            "channel_id": message.channel.id,
                            "trace_id": trace_id,
                            "user_query": user_query,
                            "tool_plan": executed_plan or tool_plan,
                            "final_response": final_response_text,
                            "is_fallback": use_fallback_prompt,
                        },
                    )
                else:
                    # RAG ë¬¸ë§¥ì´ ë…ì„±/ì•ˆì „ ë¬¸ì œë¡œ ì°¨ë‹¨ë˜ì—ˆì„ ê°€ëŠ¥ì„± -> RAG ì—†ì´ ì¬ì‹œë„
                    if rag_blocks_for_prompt:
                        logger.warning("Main ëª¨ë¸ ì‘ë‹µì´ ë¹„ì–´ìˆì–´, RAG ë¬¸ë§¥ì„ ì œì™¸í•˜ê³  ì¬ì‹œë„í•©ë‹ˆë‹¤.", extra=log_extra)
                        main_prompt_retry = self._compose_main_prompt(
                            message,
                            user_query=user_query,
                            rag_blocks=[], # RAG ì œê±°
                            tool_results_block=tool_results_str if tool_results_str else None,
                        )
                        self._debug(f"[Main Retry] user_prompt={self._truncate_for_debug(main_prompt_retry)}", log_extra)
                        retry_response = await self._safe_generate_content(
                            main_model, 
                            main_prompt_retry, 
                            log_extra,
                            generation_config=genai.types.GenerationConfig(temperature=config.AI_TEMPERATURE)
                        )
                        
                        retry_text = ""
                        if retry_response and retry_response.parts:
                            try:
                                retry_text = retry_response.text.strip()
                            except ValueError:
                                pass
                        
                        if retry_text:
                            await message.reply(retry_text, mention_author=False)
                            await db_utils.log_analytics(
                                self.bot.db,
                                "AI_INTERACTION",
                                {
                                    "guild_id": message.guild.id,
                                    "user_id": message.author.id,
                                    "channel_id": message.channel.id,
                                    "trace_id": trace_id,
                                    "user_query": user_query,
                                    "tool_plan": executed_plan or tool_plan,
                                    "final_response": retry_text,
                                    "is_fallback": True, # ì¬ì‹œë„ í–ˆìœ¼ë¯€ë¡œ fallback ì·¨ê¸‰
                                },
                            )
                            return
                        else:
                            logger.error("Main ëª¨ë¸ì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", extra=log_extra)
                            truncated_results = tool_results_str[:1900] if tool_results_str else "No tool results."
                            await message.reply(
                                "ëª¨ë“  ë„êµ¬ë¥¼ ì‹¤í–‰í–ˆì§€ë§Œ, ìµœì¢… ë‹µë³€ì„ ë§Œë“œëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”. ë„êµ¬ ì‘ë‹µ ìš”ì•½:\n```json\n"
                                f"{truncated_results}\n```",
                                mention_author=False,
                            )
                    else: # No RAG blocks for prompt, so no retry attempt
                        logger.error("Main ëª¨ë¸ì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì¬ì‹œë„ ì‹¤íŒ¨ í¬í•¨).", extra=log_extra)
                        truncated_results = tool_results_str[:1900] if tool_results_str else "No tool results."
                        await message.reply(
                            "ëª¨ë“  ë„êµ¬ë¥¼ ì‹¤í–‰í–ˆì§€ë§Œ, ìµœì¢… ë‹µë³€ì„ ë§Œë“œëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”. (AI ì‘ë‹µ ì—†ìŒ)\n```json\n"
                            f"{truncated_results}\n```",
                            mention_author=False,
                        )


            except Exception as e:
                logger.error(f"ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜: {e}", exc_info=True, extra=log_extra)
                await message.reply(config.MSG_AI_ERROR, mention_author=False)
            finally:
                self._debug(f"--- ì—ì´ì „íŠ¸ ì„¸ì…˜ ì¢…ë£Œ trace_id={trace_id}", log_extra)
    async def _get_recent_history(self, message: discord.Message, rag_prompt: str) -> list:
        """ëª¨ë¸ì— ì „ë‹¬í•  ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ ì±„ë„ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        history_limit = 6 if rag_prompt else 12
        history = []
        
        async for msg in message.channel.history(limit=history_limit + 1):
            if msg.id == message.id: continue
            role = 'model' if msg.author.id == self.bot.user.id else 'user'
            content = msg.content[:2000]
            history.append({'role': role, 'parts': [content]})

        history.reverse()
        return history

    async def should_proactively_respond(self, message: discord.Message) -> bool:
        """ë´‡ì´ ëŒ€í™”ì— ëŠ¥ë™ì ìœ¼ë¡œ ì°¸ì—¬í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ê²Œì´íŠ¸í‚¤í¼ ë¡œì§ì…ë‹ˆë‹¤."""
        conf = config.AI_PROACTIVE_RESPONSE_CONFIG
        if not conf.get("enabled"): return False
        if not self._message_has_valid_mention(message):
            # ë©˜ì…˜ì´ ì—†ë‹¤ë©´ ì–´ë–¤ ê²½ìš°ì—ë„ Gemini í˜¸ì¶œì„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤.
            return False

        now = time.time()
        if (now - self.proactive_cooldowns.get(message.channel.id, 0)) < conf.get("cooldown_seconds", 90): return False
        if len(message.content) < conf.get("min_message_length", 10): return False
        if not any(keyword in message.content.lower() for keyword in conf.get("keywords", [])): return False
        if random.random() > conf.get("probability", 0.1): return False

        log_extra = {'guild_id': message.guild.id, 'channel_id': message.channel.id}
        try:
            history_msgs = [f"User({msg.author.display_name}): {msg.content}" async for msg in message.channel.history(limit=conf.get("look_back_count", 5))]
            history_msgs.reverse()
            conversation_context = "\n".join(history_msgs)
            gatekeeper_prompt = f"""{conf['gatekeeper_persona']}\n\n--- ìµœê·¼ ëŒ€í™” ë‚´ìš© ---\n{conversation_context}\n---\nì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€: \"{message.content}\"\n---\n\nì, íŒë‹¨í•´. Yes or No?"""

            lite_model = genai.GenerativeModel(config.AI_INTENT_MODEL_NAME)
            response = await self._safe_generate_content(lite_model, gatekeeper_prompt, log_extra)

            if response and "YES" in response.text.strip().upper():
                self.proactive_cooldowns[message.channel.id] = now
                return True
        except Exception as e:
            logger.error(f"ê²Œì´íŠ¸í‚¤í¼ AI ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True, extra=log_extra)

        return False

    async def get_recent_conversation_text(self, guild_id: int, channel_id: int, look_back: int = 20) -> str:
        """DBì—ì„œ ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤ (ìš”ì•½ ê¸°ëŠ¥ìš©)."""
        if not self.bot.db: return ""
        query = "SELECT user_name, content FROM conversation_history WHERE guild_id = ? AND channel_id = ? AND is_bot = 0 ORDER BY created_at DESC LIMIT ?"
        try:
            async with self.bot.db.execute(query, (guild_id, channel_id, look_back)) as cursor:
                rows = await cursor.fetchall()
            if not rows: return ""
            rows.reverse()
            return "\n".join([f"User({row['user_name']}): {row['content']}" for row in rows])
        except Exception as e:
            logger.error(f"ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ ì¤‘ DB ì˜¤ë¥˜: {e}", exc_info=True)
            return ""

    async def generate_system_alert_message(self, channel_id: int, alert_context: str, alert_title: str | None = None) -> str | None:
        """ì£¼ê¸°ì  ì•Œë¦¼ ë“± ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ AI ë§íˆ¬ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤."""
        if not self.is_ready:
            return None

        log_extra = {'channel_id': channel_id, 'alert_title': alert_title}

        try:
            channel_config = config.CHANNEL_AI_CONFIG.get(channel_id, {})
            persona = channel_config.get('persona', config.DEFAULT_TSUNDERE_PERSONA)
            rules = channel_config.get('rules', config.DEFAULT_TSUNDERE_RULES)

            system_prompt = (
                f"{persona}\n\n{rules}\n\n"
                "### ì¶”ê°€ ì§€ì¹¨\n"
                "- ì§€ê¸ˆì€ ì„œë²„ êµ¬ì„±ì›ì—ê²Œ ì „ë‹¬í•  ì‹œìŠ¤í…œ ê³µì§€ë¥¼ ì‘ì„±í•˜ëŠ” ì¤‘ì´ë‹¤.\n"
                "- í•µì‹¬ ì •ë³´ëŠ” ë¹ ëœ¨ë¦¬ì§€ ë§ë˜ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•œë‹¤.\n"
                "- í•„ìš” ì‹œ ê°€ë²¼ìš´ ì´ëª¨ì§€ í•œë‘ ê°œë§Œ ì‚¬ìš©í•˜ê³ , ê³¼í•œ ì¥ì‹ì€ í”¼í•œë‹¤.\n"
                "- ë§ˆì§€ë§‰ì—ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ í–‰ë™ì„ ì´‰êµ¬í•˜ê±°ë‚˜ ê²©ë ¤í•˜ëŠ” ë§ì„ ë§ë¶™ì¸ë‹¤."
            )

            user_prompt = (
                "ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„œë²„ì— ì „ë‹¬í•  ê³µì§€ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì¤˜.\n"
                f"- ì•Œë¦¼ ì£¼ì œ: {alert_title or 'ì¼ë°˜ ì•Œë¦¼'}\n"
                f"- ì „ë‹¬í•  ë‚´ìš©: {alert_context}\n\n"
                "ê³µì§€ ë¬¸êµ¬ëŠ” ë§ˆì‚¬ëª½ì˜ ë§íˆ¬ë¥¼ ìœ ì§€í•´ ì£¼ê³ , ë„ˆë¬´ ì¥í™©í•˜ì§€ ì•Šê²Œ ì‘ì„±í•´ì¤˜."
            )

            # 1. CometAPI ìš°ì„  ì‚¬ìš©
            if self.use_cometapi:
                alert_message = await self._cometapi_generate_content(
                    system_prompt, 
                    user_prompt, 
                    log_extra
                )
            
            # 2. ì‹¤íŒ¨ ì‹œ Gemini í´ë°±
            if not alert_message and self.gemini_configured and genai:
                model = genai.GenerativeModel(
                    model_name=config.AI_RESPONSE_MODEL_NAME,
                    system_instruction=system_prompt,
                )
                response = await self._safe_generate_content(
                    model, 
                    user_prompt, 
                    log_extra, 
                    generation_config=genai.types.GenerationConfig(temperature=config.AI_TEMPERATURE)
                )
                if response and response.text:
                    alert_message = response.text.strip()

            if alert_message and len(alert_message) > config.AI_RESPONSE_LENGTH_LIMIT:
                alert_message = alert_message[:config.AI_RESPONSE_LENGTH_LIMIT].rstrip()
            return alert_message

        except Exception as e:
            logger.error(
                "ì‹œìŠ¤í…œ ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜: %s",
                e,
                exc_info=True,
                extra=log_extra,
            )

        return None

    async def generate_creative_text(self, channel: discord.TextChannel, author: discord.User, prompt_key: str, context: dict) -> str:
        """`!ìš´ì„¸`, `!ë­í‚¹` ë“± íŠ¹ì • ëª…ë ¹ì–´ì— ëŒ€í•œ ì°½ì˜ì ì¸ AI ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.is_ready: return config.MSG_AI_ERROR
        log_extra = {'guild_id': channel.guild.id, 'user_id': author.id, 'prompt_key': prompt_key}

        try:
            prompt_template = config.AI_CREATIVE_PROMPTS.get(prompt_key)
            if not prompt_template: return config.MSG_CMD_ERROR

            user_prompt = prompt_template.format(**context)
            system_prompt = f"{config.CHANNEL_AI_CONFIG.get(channel.id, {}).get('persona', '')}\n\n{config.CHANNEL_AI_CONFIG.get(channel.id, {}).get('rules', '')}"

            # [FIX] ëª…ë ¹ì–´ë¡œ í˜¸ì¶œëœ ê²½ìš° ë©˜ì…˜ ì •ì±… ë¬´ì‹œ (ê°€ë“œ ì œê±°)
            if config.MENTION_GUARD_SNIPPET in system_prompt:
                system_prompt = system_prompt.replace(config.MENTION_GUARD_SNIPPET, "")

            response_text = None

            # 1. CometAPI ìš°ì„  ì‚¬ìš©
            if self.use_cometapi:
                response_text = await self._cometapi_generate_content(
                    system_prompt,
                    user_prompt,
                    log_extra
                )

            # 2. ì‹¤íŒ¨ ì‹œ Gemini í´ë°±
            if not response_text and self.gemini_configured and genai:
                 model = genai.GenerativeModel(model_name=config.AI_RESPONSE_MODEL_NAME, system_instruction=system_prompt)
                 response = await self._safe_generate_content(
                     model, 
                     user_prompt, 
                     log_extra,
                     generation_config=genai.types.GenerationConfig(temperature=config.AI_TEMPERATURE)
                 )
                 if response and response.text:
                      response_text = response.text.strip()

            return response_text if response_text else config.MSG_AI_ERROR
        except KeyError as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… ì¤‘ í‚¤ ì˜¤ë¥˜: '{prompt_key}' í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸({e})ê°€ ì—†ìŠµë‹ˆë‹¤.", extra=log_extra)
            return config.MSG_CMD_ERROR
        except Exception as e:
            logger.error(f"Creative text ìƒì„± ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜: {e}", exc_info=True, extra=log_extra)
            return config.MSG_AI_ERROR

    async def extract_ticker_with_llm(self, query: str) -> str | None:
        """
        ì‚¬ìš©ì ìì—°ì–´ ì¿¼ë¦¬ì—ì„œ Yahoo Finance í˜¸í™˜ í‹°ì»¤ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ì˜ˆ: "ë¹„íŠ¸ì½”ì¸ ì–¼ë§ˆì•¼?" -> "BTC-USD"
            "ì‚¼ì„±ì „ì ì£¼ê°€" -> "005930.KS"
            "ì• í”Œ ì‹œì„¸" -> "AAPL"
        """
        if not self.use_cometapi:
             # CometAPI ì—†ìœ¼ë©´ ì‚¬ìš© ë¶ˆê°€ (í˜¹ì€ Gemini í´ë°± ê°€ëŠ¥í•˜ì§€ë§Œ ìƒëµ)
             return None

        system_prompt = (
            "You are a specialized assistant that extracts stock/crypto ticker symbols from user queries.\n"
            "The user will ask about a stock price in Korean or English.\n"
            "You must identify the correct Yahoo Finance compatible ticker symbol.\n"
            "Rules:\n"
            "1. Return ONLY the ticker symbol. Do not write any other text.\n"
            "2. For Korean stocks, append '.KS' (KOSPI) or '.KQ' (KOSDAQ). e.g., Samsung -> 005930.KS\n"
            "3. For US stocks, use the standard ticker. e.g., Apple -> AAPL\n"
            "4. For Crypto, use common pairs. e.g., Bitcoin -> BTC-USD, Ethereum -> ETH-USD\n"
            "5. If the company is not found or ambiguous, return 'NONE'."
        )
        
        user_prompt = f"Query: {query}\nTicker:"
        
        try:
            ticker = await self._cometapi_generate_content(
                system_prompt,
                user_prompt,
                log_extra={'mode': 'ticker_extraction'}
            )
            if ticker and "NONE" not in ticker:
                clean_ticker = ticker.strip().replace("'", "").replace('"', '').upper()
                return clean_ticker
            return None
        except Exception as e:
            logger.error(f"Ticker extraction failed: {e}")
            return None


async def setup(bot: commands.Bot):
    """Cogë¥¼ ë´‡ì— ë“±ë¡í•˜ëŠ” í•¨ìˆ˜"""
    await bot.add_cog(AIHandler(bot))
