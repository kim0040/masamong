# -*- coding: utf-8 -*-
"""
KakaoTalk Embedding Generator V2 (Session Summary Edition)

This script upgrades the embedding generation process by:
1. Grouping messages into 'Sessions' based on a 1-hour silence gap.
2. Summarizing each session using DeepSeek (via CometAPI) to extract key points and context.
3. Embedding the 'Summary' instead of raw text for better semantic retrieval.
4. Saving the original text in metadata for full context display.

Usage:
    python scripts/generate_kakao_embeddings_v2.py
"""

import os
import sys
import json
import asyncio
import argparse
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any

import pandas as pd
import numpy as np
from tqdm.asyncio import tqdm
from sentence_transformers import SentenceTransformer
from openai import AsyncOpenAI

# Add project root to path for importing config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import config
except ImportError:
    config = type('Config', (), {})

# Configure Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("KakaoEmbedderV2")

# Constants
DEFAULT_MODEL_NAME = "dragonkue/multilingual-e5-small-ko-v2"
SESSION_GAP_MINUTES = 60  # 1 hour
# Summarization Model Configs
SUMMARIZATION_MODELS = {
    "1": {
        "name": "DeepSeek-V3.2-Exp-nothinking",
        "price_input": 0.27,
        "price_output": 0.432,
        "desc": "í‘œì¤€í˜• (DeepSeek V3.2)"
    },
    "2": {
        "name": "gpt-5-nano-2025-08-07",
        "price_input": 0.05,
        "price_output": 0.40,
        "desc": "ì ˆì•½í˜• (GPT-5 Nano)"
    }
}
EXCHANGE_RATE = 1470 

class KakaoSessionEmbedder:
    def __init__(self, embedding_model_name: str, api_key: str, base_url: str, summary_model_config: Dict[str, Any]):
        self.embedding_model_name = embedding_model_name
        self.summary_model_config = summary_model_config
        self.embedding_model = None
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)

    def load_model(self):
        """Loads the SentenceTransformer model."""
    def load_model(self):
        """Loads the SentenceTransformer model."""
        logger.info(f"Loading embedding model: {self.embedding_model_name}...")
        self.embedding_model = SentenceTransformer(self.embedding_model_name)

    def load_csv(self, path: str) -> pd.DataFrame:
        """Loads and normalizes the KakaoTalk CSV."""
        logger.info(f"Loading CSV: {path}")
        try:
            df = pd.read_csv(path)
            
            # Normalize columns
            col_map = {}
            for col in df.columns:
                l_col = col.lower().strip()
                if l_col in ['date', 'time', 'timestamp', 'ë‚ ì§œ', 'ì‹œê°„']:
                    col_map[col] = 'date'
                elif l_col in ['user', 'sender', 'author', 'name', 'ë³´ë‚¸ì´', 'ì‚¬ëŒ']:
                    col_map[col] = 'user'
                elif l_col in ['message', 'content', 'text', 'msg', 'ë‚´ìš©', 'ë©”ì‹œì§€']:
                    col_map[col] = 'message'
            
            df = df.rename(columns=col_map)
            
            if not all(k in df.columns for k in ['date', 'user', 'message']):
                logger.error(f"Missing required columns. Found: {df.columns.tolist()}")
                return pd.DataFrame()
            
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            return df[['date', 'user', 'message']].fillna('')
            
        except Exception as e:
            logger.error(f"Failed to load CSV: {e}")
            return pd.DataFrame()

    def group_into_sessions(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """Groups messages into sessions based on time gaps."""
        logger.info("Grouping messages into sessions...")
        sessions = []
        current_session_msgs = []
        last_time = None
        
        for idx, row in df.iterrows():
            msg_date = row['date']
            
            is_new_session = False
            if last_time:
                diff = (msg_date - last_time).total_seconds() / 60
                if diff > SESSION_GAP_MINUTES:
                    is_new_session = True
            
            if is_new_session and current_session_msgs:
                sessions.append(self._format_session(current_session_msgs))
                current_session_msgs = []
            
            current_session_msgs.append({
                'user': str(row['user']),
                'message': str(row['message']),
                'date': msg_date
            })
            last_time = msg_date
            
        if current_session_msgs:
            sessions.append(self._format_session(current_session_msgs))
            
        logger.info(f"Found {len(sessions)} distinct conversation sessions.")
        return sessions

    def _format_session(self, messages: List[Dict]) -> Dict[str, Any]:
        """Formats a list of messages into a single text block."""
        lines = []
        start_time = messages[0]['date']
        end_time = messages[-1]['date']
        
        merged = []
        prev_user = None
        current_block = []
        
        for msg in messages:
            user = msg['user']
            text = msg['message']
            if user == prev_user:
                current_block.append(text)
            else:
                if prev_user:
                    merged.append(f"{prev_user}: {' '.join(current_block)}")
                prev_user = user
                current_block = [text]
        if prev_user:
            merged.append(f"{prev_user}: {' '.join(current_block)}")
            
        full_text = "\n".join(merged)
        
        return {
            'start_date': start_time,
            'end_date': end_time,
            'full_text': full_text,
            'message_count': len(messages)
        }

    async def summarize_session(self, session_text: str, semaphore: asyncio.Semaphore) -> str:
        """Uses DeepSeek/GPT-5 to summarize the session with key details, respecting rate limits."""
        
        # [Speed Optimization] ì§§ì€ ì„¸ì…˜ì€ êµ³ì´ API í˜¸ì¶œë„, ëŒ€ê¸°ë„ í•„ìš” ì—†ìŒ (ì¦‰ì‹œ ì²˜ë¦¬)
        if len(session_text) < 200:
            return session_text

        retries = 0
        max_retries = 3
        backoff = 2.0

        async with semaphore:
            while retries < max_retries:
                # Truncate if too long (strict limit)
                truncated_text = session_text
                if len(session_text) > 20000:
                    truncated_text = session_text[:20000] + "\n...(ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìƒëµë¨)"

                system_prompt = """ì—­í• : ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë‚´ìš© ìš”ì•½ê°€ (ë¹„ìš© ì ˆê° ëª¨ë“œ)
ëª©í‘œ: ê²€ìƒ‰ìš© í•µì‹¬ ì •ë³´ ì¶”ì¶œ. 600ì ì´ë‚´, í•µì‹¬ë§Œ.
ì£¼ì˜: ì¸ì‚¬ë§, ê°íƒ„ì‚¬, ë¬´ì˜ë¯¸í•œ ë°˜ë³µ(ã…‹ã…‹ ë“±)ì€ ì™„ì „íˆ ì œê±°."""

                user_prompt = f"""[ì…ë ¥ ë°ì´í„°]
{truncated_text}

[ì¶œë ¥ í˜•ì‹]
ìš”ì•½: (ëŒ€í™”ì˜ ì£¼ì œì™€ ê²°ë¡ ì„ 3-4ë¬¸ì¥ ê±´ì¡°ì²´ë¡œ ì‘ì„±)
í‚¤ì›Œë“œ: (ë‚ ì§œ, ì‹œê°„, ì¥ì†Œ, URL, ê³ ìœ ëª…ì‚¬, ìˆ«ì, ì£¼ì‹ì¢…ëª© ë“± ê²€ìƒ‰ì— ê±¸ë¦´ë§Œí•œ ë‹¨ì–´ë§Œ ë‚˜ì—´)"""

                try:
                    # [Model Compatibility]
                    # ìµœì‹  OpenAI SDKëŠ” 'gpt-5', 'o1' ë“±ì´ ì´ë¦„ì— í¬í•¨ë˜ë©´ max_tokens -> max_completion_tokensë¡œ ìë™ ë³€í™˜í•¨.
                    # í•˜ì§€ë§Œ CometAPI(Relay)ëŠ” ì•„ì§ max_tokensë§Œ ì¸ì‹í•˜ì—¬ 400 ì—ëŸ¬ ë°œìƒ.
                    # ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ max_tokensë¥¼ kwargsê°€ ì•„ë‹Œ extra_bodyë¡œ ì§ì ‘ ì£¼ì…í•˜ì—¬ ë³€í™˜ì„ ìš°íšŒí•¨.
                    
                    # Common args
                    api_args = {
                        "model": self.summary_model_config['name'],
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        # "max_tokens": 400  <-- Do NOT pass this directly if model name contains 'gpt-5' or 'o1'
                        "extra_body": {"max_tokens": 400}
                    }

                    response = await self.client.chat.completions.create(**api_args)
                    summary = response.choices[0].message.content.strip()
                
                    # Preview Verification & Fallback
                    if not summary:
                        tqdm.write("âš ï¸ [Warning] ëª¨ë¸ ì‘ë‹µì´ ë¹„ì–´ìˆìŒ -> ì›ë¬¸ ì•ë¶€ë¶„ ì‚¬ìš©")
                        summary = session_text[:500].replace('\n', ' ')

                    # Preview: í•œ ì¤„ë¡œ ê³µë°± ì œê±°í•´ì„œ ê¹”ë”í•˜ê²Œ ì¶œë ¥
                    preview = summary.replace('\n', ' ')[:80]
                    tqdm.write(f"ğŸ“ {preview}...") 
                    return summary
                except Exception as e:
                    if "429" in str(e):
                        logger.warning(f"Rate limited. Sleeping {backoff}s...")
                        await asyncio.sleep(backoff)
                        backoff *= 2
                        retries += 1
                        continue
                    else:
                        logger.error(f"Summarization failed: {e}")
                        # Fallback for API Error
                        return session_text[:500].replace('\n', ' ')
            
            return session_text[:500].replace('\n', ' ')

    async def _summarize_with_progress(self, idx, session, semaphore, pbar, output_dir):
        """Wrapper to update progress bar and save incremental checkpoint."""
        # Note: summarize_session no longer takes date, we attach it here for embedding
        summary = await self.summarize_session(session['full_text'], semaphore)
        
        # [Date Injection] ì„ë² ë”© í…ìŠ¤íŠ¸ì— ë‚ ì§œë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
        date_str = str(session['start_date'])[:10] # YYYY-MM-DD only
        embedding_text = f"passage: [{date_str}] {summary}"
        
        result = {
            'id': idx,
            'summary': summary,
            'embedding_text': embedding_text,
            'original_text': session['full_text'],
            'start_date': str(session['start_date']),
            'end_date': str(session['end_date']),
            'message_count': session['message_count']
        }
        
        pbar.update(1)
        return result

    async def process(self, input_path: str, output_dir: str, reset: bool = False, confirmed: bool = False):
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        checkpoint_path = output_path / "checkpoint.jsonl"
        
        # 1. Load Checkpoint (unless reset is requested)
        completed_indices = set()
        completed_results = []
        
        if checkpoint_path.exists():
            if reset:
                print(f"ğŸ—‘ï¸ [ì´ˆê¸°í™”] ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤: {checkpoint_path}")
                # We don't delete the file immediately to be safe, just don't load it.
                # However, we should properly clear it if we start writing.
                with open(checkpoint_path, 'w') as f: # Clear file
                    pass
            else:
                print(f"\nğŸ“‚ [ì²´í¬í¬ì¸íŠ¸ ë°œê²¬] {checkpoint_path}")
                try:
                    with open(checkpoint_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                data = json.loads(line)
                                completed_indices.add(data['id'])
                                completed_results.append(data)
                    print(f"âœ… {len(completed_indices)}ê°œ ì„¸ì…˜ì€ ì´ë¯¸ ì²˜ë¦¬ë¨. ì´ì–´ì„œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                    
                    if not confirmed:
                        q = input("ğŸ”„ ê¸°ì¡´ ì‘ì—…ì„ ì´ì–´í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y=ì´ì–´í•˜ê¸° / n=ì²˜ìŒë¶€í„° ë‹¤ì‹œ): ").strip().lower()
                        if q in ['n', 'no', 'new']:
                            print("ğŸ—‘ï¸ ì‘ì—…ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")
                            completed_indices = set()
                            completed_results = []
                            with open(checkpoint_path, 'w') as f: pass # Clear
                except Exception as e:
                    logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 2. Load Data
        df = self.load_csv(input_path)
        if df.empty: return

        # 3. Group into Sessions
        sessions = self.group_into_sessions(df)
        total_sessions = len(sessions)
        
        # Filter already processed sessions
        remaining_tasks = []
        for i, s in enumerate(sessions):
            if i not in completed_indices:
                remaining_tasks.append((i, s))

        # Pricing
        total_remaining_sessions = len(remaining_tasks)
        if total_remaining_sessions == 0:
            print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„ë² ë”© ìƒì„± ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
        else:
            total_chars = sum(len(s['full_text']) for i, s in remaining_tasks)
            est_input_tokens = total_chars / 2 
            est_output_tokens = total_remaining_sessions * 200 

            model_name = self.summary_model_config['name']
            p_in = self.summary_model_config['price_input']
            p_out = self.summary_model_config['price_output']
            
            cost_input = (est_input_tokens / 1_000_000) * p_in
            cost_output = (est_output_tokens / 1_000_000) * p_out
            total_est_cost = cost_input + cost_output
            
            print("\n" + "="*50)
            print(f"ğŸ“Š [ë‚¨ì€ ì‘ì—… ë¹„ìš©/ê·œëª¨ ë¶„ì„]")
            print(f"ë‚¨ì€ ì„¸ì…˜ ìˆ˜     : {total_remaining_sessions:,} ê°œ (ì´ {total_sessions:,} ê°œ)")
            print(f"ì´ ì…ë ¥ ê¸€ì ìˆ˜ : {total_chars:,} ì")
            print(f"ì˜ˆìƒ ì…ë ¥ í† í°  : ì•½ {int(est_input_tokens):,} tokens")
            print(f"ì˜ˆìƒ ì¶œë ¥ í† í°  : ì•½ {int(est_output_tokens):,} tokens")
            print("-" * 30)
            print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©    : ${total_est_cost:.4f} (ì•½ {int(total_est_cost * EXCHANGE_RATE)}ì›)")
            print(f"* ì‚¬ìš© ëª¨ë¸: {model_name}")
            print("="*50 + "\n")

            if not confirmed:
                user_input = input("ğŸ’¡ ìœ„ ì˜ˆìƒ ë¹„ìš©ìœ¼ë¡œ ì‘ì—…ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
                if user_input.lower() not in ['y', 'yes']:
                    print("ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                    return

        # 4. Summarize Sessions (Async with Semaphore)
        # Concurrency Increased: 5
        semaphore = asyncio.Semaphore(5) 
        
        logger.info(f"Summarizing sessions using {self.summary_model_config['name']} (Concurrency=5)...")
        
        pbar = tqdm(total=total_sessions, initial=len(completed_indices), desc="Processing Sessions")
        
        # Run remaining tasks
        # We need to save incrementally
        chunk_size = 10  # Save every 10 items
        
        tasks_iter = iter(remaining_tasks)
        
        while True:
            chunk = []
            try:
                for _ in range(chunk_size):
                    chunk.append(next(tasks_iter))
            except StopIteration:
                pass
            
            if not chunk:
                break
                
            chunk_tasks = [self._summarize_with_progress(idx, s, semaphore, pbar, output_dir) for idx, s in chunk]
            results = await asyncio.gather(*chunk_tasks)
            
            # Save Checkpoint immediately
            with open(checkpoint_path, 'a', encoding='utf-8') as f:
                for res in results:
                    f.write(json.dumps(res, ensure_ascii=False) + "\n")
            
            completed_results.extend(results)

        pbar.close()
        
        # Sort results by ID to restore order
        completed_results.sort(key=lambda x: x['id'])
        summarized_sessions = completed_results

        # 4. Generate Embeddings
        self.load_model()
        logger.info("Generating embeddings...")
        
        texts_to_embed = [s['embedding_text'] for s in summarized_sessions]
        embeddings = self.embedding_model.encode(texts_to_embed, normalize_embeddings=True, show_progress_bar=True)
        
        # 5. Save Results
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        np_embeddings = np.array(embeddings, dtype=np.float32)
        np.save(output_path / "vectors.npy", np_embeddings)
        
        metadata = []
        for i, s in enumerate(summarized_sessions):
            metadata.append({
                "id": i,
                "text": f"[ëŒ€í™” ì¼ì‹œ: {s['start_date']}]\n\nğŸ“Œ {s['summary']}\n\n---\n[ìƒì„¸ ë‚´ìš©]\n{s['original_text']}",
                "summary": s['summary'],
                "start_date": s['start_date'],
                "message_count": s['message_count']
            })
            
        with open(output_path / "metadata.json", "w", encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
        logger.info(f"âœ… Analysis & Embedding Complete!")
        logger.info(f"Saved {len(metadata)} items to {output_path}")

def main():
    parser = argparse.ArgumentParser(description="KakaoEmbedder V2 (Session Summary)")
    parser.add_argument("--input", "-i", type=str, default="data/kakao_raw/kakao_chat.csv")
    parser.add_argument("--output", "-o", type=str, default="data/kakao_store_v2")
    parser.add_argument("--model", "-m", type=str, default=DEFAULT_MODEL_NAME)
    parser.add_argument("--key", "-k", type=str, help="CometAPI Key (optional if in env)")
    parser.add_argument("--yes", "-y", action="store_true", help="Skip confirmation")
    parser.add_argument("--sum-model", type=str, help="Summarization Model Key (1 or 2)")
    parser.add_argument("--reset", action="store_true", help="Ignore checkpoint and restart")
    args = parser.parse_args()
    
    # --- Interactive Setup ---
    # 0. Summarization Model Selection
    selected_model_config = None
    if args.sum_model and args.sum_model in SUMMARIZATION_MODELS:
        selected_model_config = SUMMARIZATION_MODELS[args.sum_model]
    else:
        print("\nğŸ¤– [ìš”ì•½ ëª¨ë¸ ì„ íƒ]")
        for key, conf in SUMMARIZATION_MODELS.items():
            print(f"  {key}. {conf['name']} ({conf['desc']})")
            print(f"     â””â”€ Input ${conf['price_input']}/M, Output ${conf['price_output']}/M")
        
        while True:
            choice = input(f"ğŸ‘‰ ëª¨ë¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (ê¸°ë³¸ê°’ 1): ").strip()
            if not choice:
                choice = "1"
            if choice in SUMMARIZATION_MODELS:
                selected_model_config = SUMMARIZATION_MODELS[choice]
                break
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    # 1. API Key Setup
    api_key = args.key or os.environ.get("COMETAPI_KEY") or getattr(config, 'COMETAPI_KEY', None)
    if not api_key:
        print("\nğŸ”‘ [API í‚¤ ì„¤ì •]")
        print("CometAPI Keyê°€ í™˜ê²½ ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
        api_key = input("ğŸ‘‰ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì…ë ¥ ë‚´ìš©ì€ ìˆ¨ê²¨ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤): ").strip()
        if not api_key:
            logger.error("API Keyê°€ ì…ë ¥ë˜ì§€ ì•Šì•„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

    # 2. Input File Selection
    input_path = args.input
    # ê¸°ë³¸ê°’ì´ê³  ì‹¤ì œ íŒŒì¼ì´ ì—†ë‹¤ë©´, í˜¹ì€ ì‚¬ìš©ìê°€ ì„ íƒí•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª©ë¡ ë³´ì—¬ì£¼ê¸° ë£¨í‹´
    # (ë‹¨, argsë¡œ ëª…ì‹œì ìœ¼ë¡œ ê²½ë¡œë¥¼ ì¤¬ë‹¤ë©´ ê·¸ê²ƒì„ ìš°ì„ )
    if input_path == "data/kakao_raw/kakao_chat.csv" and not os.path.exists(input_path):
        # Scan directory
        raw_dir = Path("data/kakao_raw")
        csv_files = list(raw_dir.glob("*.csv")) if raw_dir.exists() else []
        
        if not csv_files:
            print(f"\nğŸ“‚ [íŒŒì¼ ì„ íƒ] '{raw_dir}' ê²½ë¡œì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            input_path = input("ğŸ‘‰ ë¶„ì„í•  ì¹´ì¹´ì˜¤í†¡ CSV íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        else:
            print(f"\nğŸ“‚ [íŒŒì¼ ì„ íƒ] '{raw_dir}' ê²½ë¡œì—ì„œ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:")
            for idx, f in enumerate(csv_files, 1):
                print(f"  {idx}. {f.name}")
            print("  0. ì§ì ‘ ê²½ë¡œ ì…ë ¥")
            
            while True:
                try:
                    choice = input(f"ğŸ‘‰ ì‘ì—…í•  íŒŒì¼ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1~{len(csv_files)}, 0=ì§ì ‘ì…ë ¥): ")
                    idx = int(choice)
                    if idx == 0:
                        input_path = input("ğŸ‘‰ íŒŒì¼ ê²½ë¡œ ì…ë ¥: ").strip()
                        break
                    if 1 <= idx <= len(csv_files):
                        input_path = str(csv_files[idx-1])
                        break
                except ValueError:
                    pass
                print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if not os.path.exists(input_path):
        logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        return

    print(f"\nâœ… ì„ íƒëœ íŒŒì¼: {input_path}")
    print(f"âœ… ì‚¬ìš© API Key: {api_key[:8]}..." if api_key else "âœ… API Key í™•ì¸ë¨")
    base_url = os.environ.get("COMETAPI_BASE_URL") or getattr(config, 'COMETAPI_BASE_URL', "https://api.cometapi.com/v1")
    
    if not api_key:
        logger.error("API Key not found. Please provide via --key or set COMETAPI_KEY env var.")
        return

    embedder = KakaoSessionEmbedder(args.model, api_key, base_url, selected_model_config)
    asyncio.run(embedder.process(input_path, args.output, args.reset, args.yes))

if __name__ == "__main__":
    main()
