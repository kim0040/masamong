# -*- coding: utf-8 -*-
"""
ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë‚´ìš©(.csv)ì„ ë¡œì»¬ì—ì„œ ë¯¸ë¦¬ ì„ë² ë”©í•˜ì—¬ ì„œë²„ì— ì—…ë¡œë“œí•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë¡œì»¬ PC(ê³ ì„±ëŠ¥)ì—ì„œ ì‹¤í–‰í•œ í›„, ìƒì„±ëœ ê²°ê³¼ë¬¼ë§Œ ì„œë²„ë¡œ ì˜®ê¸°ì„¸ìš”.

[ì‚¬ìš© ë°©ë²•]
1. CSV íŒŒì¼ ì¤€ë¹„
   - ì»¬ëŸ¼: date, user, message (ë˜ëŠ” sender, content ë“± ìœ ì—°í•˜ê²Œ ì²˜ë¦¬í•¨)
   - ìœ„ì¹˜: data/kakao_raw/kakao_chat.csv (ê¸°ë³¸ê°’)

2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
   python scripts/generate_kakao_embeddings.py

3. ìƒì„±ëœ íŒŒì¼(data/kakao_store/)ì„ ì„œë²„ì˜ ë™ì¼í•œ ê²½ë¡œë¡œ ì—…ë¡œë“œ
"""

import sys
import os
import glob
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("KakaoEmbedder")

# ì„¤ì •ê°’ (config.py ì˜ì¡´ì„± ì œê±°ë¥¼ ìœ„í•´ ì§ì ‘ ì •ì˜)
DEFAULT_MODEL_NAME = "dragonkue/multilingual-e5-small-ko-v2"
CHUNK_SIZE = 12  # config.py: CONVERSATION_WINDOW_SIZE
CHUNK_STRIDE = 6 # config.py: CONVERSATION_WINDOW_STRIDE
MAX_MESSAGES_PER_CHUNK = 15 # ì²­í‚¹ ì‹œ ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ (ì•½ 15ê°œ)
TIME_WINDOW_MINUTES = 10    # ëŒ€í™” ëŠê¹€ íŒë³„ ê¸°ì¤€

def load_csv_flexible(path: str) -> pd.DataFrame:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì¹´ì¹´ì˜¤í†¡ CSVë¥¼ ì½ì–´ í‘œì¤€ ì»¬ëŸ¼(date, user, message)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        df = pd.read_csv(path)
        
        # ì»¬ëŸ¼ ì •ê·œí™”
        col_map = {}
        for col in df.columns:
            l_col = col.lower().strip()
            if l_col in ['date', 'time', 'timestamp', 'ë‚ ì§œ', 'ì‹œê°„']:
                col_map[col] = 'date'
            elif l_col in ['user', 'sender', 'author', 'name', 'ë³´ë‚¸ì´', 'ì‚¬ëŒ']:
                col_map[col] = 'user'
            elif l_col in ['message', 'content', 'text', 'msg', 'ë‚´ìš©', 'ë©”ì‹œì§€']:
                col_map[col] = 'message'
                
        df = df.rename(columns=col_map)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required = ['date', 'user', 'message']
        if not all(c in df.columns for c in required):
            logger.error(f"CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (ë°œê²¬ëœ ì»¬ëŸ¼: {df.columns.tolist()})")
            return pd.DataFrame()
            
        # ë‚ ì§œ ì •ë ¬
        try:
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
        except Exception:
            pass # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•´ë„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            
        return df[['date', 'user', 'message']].fillna('')
        
    except Exception as e:
        logger.error(f"CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def chunk_conversations(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """ëŒ€í™” ë‚´ìš©ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤. (í™”ì ë³‘í•© ì ìš©)"""
    chunks = []
    current_chunk_msgs = []
    last_time = None
    
    logger.info("ëŒ€í™” ë‚´ìš© ì²­í‚¹ ì¤‘...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        msg_date = row['date']
        
        # 1. ì‹œê°„ ì°¨ì´ì— ë”°ë¥¸ ë¶„í• 
        is_time_split = False
        if isinstance(msg_date, (datetime, pd.Timestamp)) and isinstance(last_time, (datetime, pd.Timestamp)):
            diff = (msg_date - last_time).total_seconds() / 60
            if diff > TIME_WINDOW_MINUTES:
                is_time_split = True
        
        # 2. ì²­í¬ í¬ê¸° ì œí•œì— ë”°ë¥¸ ë¶„í• 
        if len(current_chunk_msgs) >= MAX_MESSAGES_PER_CHUNK or is_time_split:
            if current_chunk_msgs:
                chunks.append(format_chunk(current_chunk_msgs))
                current_chunk_msgs = []
        
        current_chunk_msgs.append({
            'user': str(row['user']),
            'message': str(row['message']),
            'date': str(row['date'])
        })
        last_time = msg_date
        
    # ë‚¨ì€ ë‚´ìš© ì²˜ë¦¬
    if current_chunk_msgs:
        chunks.append(format_chunk(current_chunk_msgs))
        
    logger.info(f"ì´ {len(chunks)}ê°œì˜ ëŒ€í™” ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return chunks

def format_chunk(messages: List[Dict[str, str]]) -> Dict[str, Any]:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    
    # 1. í…ìŠ¤íŠ¸ ë³‘í•© (ê°™ì€ í™”ìë¼ë¦¬ ë¬¶ê¸°)
    lines = []
    start_time = messages[0]['date']
    lines.append(f"[ëŒ€í™” ì‹œê°„: {start_time}]")
    
    prev_user = None
    current_block = []
    
    for msg in messages:
        user = msg['user']
        text = msg['message']
        
        if user == prev_user:
            current_block.append(text)
        else:
            if prev_user:
                merged_line = f"{prev_user}: {' '.join(current_block)}"
                lines.append(merged_line)
            prev_user = user
            current_block = [text]
            
    if prev_user:
        merged_line = f"{prev_user}: {' '.join(current_block)}"
        lines.append(merged_line)
        
    combined_text = "\n".join(lines)
    
    # E5 ëª¨ë¸ìš© Prefix
    embedding_text = f"passage: {combined_text}"
    
    return {
        "text": combined_text,
        "embedding_text": embedding_text,
        "start_date": str(start_time),
        "message_count": len(messages)
    }

def main():
    parser = argparse.ArgumentParser(description="KakaoTalk Offline Embedding Generator")
    parser.add_argument("--input", "-i", type=str, default="data/kakao_raw/kakao_chat.csv", help="Input CSV file path")
    parser.add_argument("--output", "-o", type=str, default="data/kakao_store", help="Output directory path")
    parser.add_argument("--model", "-m", type=str, default=DEFAULT_MODEL_NAME, help="HuggingFace model name")
    args = parser.parse_args()
    
    input_path = Path(args.input)
    output_dir = Path(args.output)
    
    # 1. íŒŒì¼ í™•ì¸
    if not input_path.exists():
        logger.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        logger.info("íŒ: data/kakao_raw í´ë”ë¥¼ ë§Œë“¤ê³  kakao_chat.csv íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return
        
    # 2. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 3. ë°ì´í„° ë¡œë“œ
    logger.info(f"íŒŒì¼ ë¡œë“œ ì¤‘: {input_path}")
    df = load_csv_flexible(str(input_path))
    if df.empty:
        return
    logger.info(f"ì´ {len(df)}ê°œì˜ ë©”ì‹œì§€ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")
    
    # 4. ì²­í‚¹
    chunks = chunk_conversations(df)
    if not chunks:
        logger.warning("ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    # 5. ëª¨ë¸ ë¡œë“œ
    logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ({args.model})...")
    try:
        model = SentenceTransformer(args.model)
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
        
    # 6. ì„ë² ë”© ìƒì„±
    batch_size = 32
    vectors = []
    
    logger.info("ì„ë² ë”© ìƒì„± ì‹œì‘...")
    chunk_texts = [c['embedding_text'] for c in chunks]
    
    for i in tqdm(range(0, len(chunk_texts), batch_size)):
        batch = chunk_texts[i:i+batch_size]
        # normalize_embeddings=True for cosine similarity
        emb = model.encode(batch, normalize_embeddings=True)
        vectors.extend(emb)
        
    # 7. ì €ì¥
    vectors_np = np.array(vectors, dtype=np.float32)
    
    # ë©”íƒ€ë°ì´í„°ì—ëŠ” textì™€ ê¸°íƒ€ ì •ë³´ë§Œ ì €ì¥ (ìœ ì‚¬ë„ ê²€ìƒ‰ í›„ ì›ë³¸ í…ìŠ¤íŠ¸ í‘œì‹œìš©)
    metadata = []
    for i, c in enumerate(chunks):
        metadata.append({
            "id": i,
            "text": c['text'],
            "start_date": c['start_date'],
            "message_count": c['message_count']
        })
        
    np.save(output_dir / "vectors.npy", vectors_np)
    with open(output_dir / "metadata.json", "w", encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
        
    logger.info("âœ… ì‘ì—… ì™„ë£Œ!")
    logger.info(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {output_dir.absolute()}")
    logger.info(f"   - vectors.npy (Shape: {vectors_np.shape})")
    logger.info(f"   - metadata.json ({len(metadata)} items)")
    logger.info("\nğŸ“¢ ì´ 'kakao_store' í´ë” ì „ì²´ë¥¼ ì„œë²„ì˜ ë™ì¼í•œ ìœ„ì¹˜ë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
