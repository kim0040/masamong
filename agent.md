
-----

# 마사몽 봇 기술 개선 최종 명세서 (v5.1)

## 1\. 개요: 반응적 수리에서 능동적 진화로

이 문서는 마사몽 봇의 운영 패러다임을 전환하는 것을 목표로 합니다. 단순히 발생하는 오류를 수정하는 **'반응적(Reactive) 수리'** 단계에서 벗어나, 봇의 성능을 극대화하고 지능을 향상시키는 **'능동적(Proactive) 진화'** 단계로 나아가기 위한 종합적인 기술 로드맵을 제시합니다.

### 개선의 흐름: 4단계 진화 전략

우리는 봇의 발전을 4가지 단계로 나누어 체계적으로 접근할 것입니다.

1.  **Phase 1: 생존 (Stability)**: 어떤 외부 충격에도 멈추지 않는, 절대 죽지 않는 봇을 만듭니다.
2.  **Phase 2: 성장 (Efficiency)**: 제한된 자원(무료 API 할당량)을 가장 효율적으로 사용하여 운영 비용을 최적화합니다.
3.  **Phase 3: 지능 (Intelligence)**: 주어진 기능을 넘어, 사용자의 의도를 더 깊이 파악하고 더 풍부한 정보를 제공하는 똑똑한 봇으로 만듭니다.
4.  **Phase 4: 생태계 (Ecosystem)**: 장기적인 유지보수와 확장을 고려한 견고한 코드 생태계를 구축합니다.

이 문서는 각 단계별 문제점을 상세히 분석하고, 누구나 이해하고 실행할 수 있는 명확한 해결책을 제공합니다.

-----

## Phase 1: 생존 - 안정성 확보

### 문제 1 & 4: 외부 환경 변화로 인한 봇 중단 현상

  * **현상**: Gemini API 할당량 초과, 날씨 API 응답 형식 변경, 환율 API 서버 연결 불안정 등 예측 불가능한 외부 요인으로 인해 봇이 그대로 멈춰버립니다.
  * **근본 원인**: 외부 세계(API)와의 모든 통신은 잠재적 실패 가능성을 내포하고 있음에도, 현재 코드는 \*\*'통신은 무조건 성공할 것이다'\*\*라는 낙관적인 가정 하에 설계되었습니다. `try...except`와 같은 예외 처리 방어벽이 없어, 외부에서 발생한 문제가 그대로 프로그램의 심장부까지 파고들어 시스템 전체를 중단시키는 구조입니다.
  * **해결 방안**: \*\*'모든 외부 통신은 실패할 수 있다'\*\*는 원칙을 적용하여, 모든 API 호출 지점을 `try...except` 블록으로 감싸는 '방화벽'을 구축합니다.
      * **적용 대상**: `cogs/ai_handler.py`의 `generate_gemini_response`, `utils/api_handlers/` 내 모든 API 호출 함수.
      * **핵심 로직**:
        1.  `try` 블록 안에서 API 통신을 시도합니다.
        2.  통신에 성공하면 정상적으로 데이터를 반환합니다.
        3.  `except` 블록을 통해 `ResourceExhausted`(할당량 초과), `ConnectionError`(연결 오류), `KeyError`(데이터 구조 문제) 등 발생 가능한 모든 종류의 예외를 붙잡습니다.
        4.  예외 발생 시, 오류를 로그에 상세히 기록하고(원인 추적을 위해), 함수는 프로그램에 치명적이지 않은 `None`이나 빈 리스트(`[]`) 같은 안전한 값을 반환합니다.
        5.  이 함수를 호출한 상위 코드에서는 반환된 값이 안전한 값인지(예: `if response is None:`) 반드시 확인하여, "API 통신에 실패했습니다."와 같은 대체 응답을 하도록 처리합니다.

### 문제 2: 명령어와 AI 대화의 역할 충돌

  * **현상**: `!주사위` 같은 명령어를 입력하면, 주사위 결과와 함께 AI의 불필요한 분석("주사위 명령어를 사용하셨군요.")이 함께 출력됩니다.
  * **근본 원인**: 봇의 귀 역할을 하는 `on_message` 이벤트 핸들러가 들어오는 모든 소리(메시지)를 듣고 AI에게 전달하는 **'만능 귀'** 역할을 하고 있기 때문입니다. 정해진 명령어(`!`)는 `commands` 프레임워크라는 \*\*'전문가'\*\*가 처리해야 함에도, '만능 귀'가 먼저 가로채서 처리해버리는 문제입니다.
  * **해결 방안**: `on_message` 함수에 **'교통정리'** 역할을 부여합니다. 메시지를 받자마자, 이것이 전문가(명령어 처리기)에게 가야 할 일인지, 아니면 일반 대화인지 판단하는 로직을 함수 최상단에 추가합니다.
      * **구현**: `cogs/ai_handler.py`의 `on_message` 함수 맨 처음에 `if message.content.startswith('!'): return` 과 같은 조건문을 추가합니다.
      * **효과**: 이 한 줄의 코드는 '이 메시지는 내 담당이 아니니, 나는 아무것도 하지 않고 전문가에게 넘기겠다'는 명확한 역할 분담을 만들어, 시스템의 논리적 흐름을 바로잡습니다.

-----

## Phase 2: 성장 - 효율성 극대화

### 문제 5: 비효율적인 데이터 전달로 인한 LLM 과부하

  * **현상**: API로부터 받은 거대한 JSON 원본 데이터를 아무런 가공 없이 그대로 LLM에게 전달합니다. 이는 LLM에게 불필요한 정보를 해석하게 만들어 토큰 사용량을 낭비하고 응답 속도를 저하시킵니다.
  * **근본 원인**: **'데이터 전처리(Pre-processing)'** 단계의 부재. LLM은 잘 정돈된 요리 재료(요약된 텍스트)를 받았을 때 최고의 성능을 내지만, 현재 우리는 손질되지 않은 식재료(원본 JSON)를 통째로 던져주고 있습니다.
  * **해결 방안**: 각 API 핸들러에 \*\*'LLM 전용 데이터 정제기'\*\*를 구현합니다. 이 정제기는 원본 데이터에서 핵심 정보만 추출하고, 코드나 숫자를 사람이 이해할 수 있는 언어로 번역하여 LLM에게 전달합니다.

| **API** | **Before (LLM 입력)** | **After (LLM 입력)** | **효과** |
| :--- | :--- | :--- | :--- |
| **날씨** | `...{"category":"SKY","fcstValue":"3"}...` | `"하늘 상태는 '구름많음'입니다."` | 토큰 90% 이상 절감, 명확성 증대 |
| **환율** | `...{"ttb":"1300.50","tts":"1350.20"}...` | `"은행에서 현찰을 살 때(TCB)와 팔 때(TTS)의 환율 정보가 포함되어 있습니다."` | 불필요한 소수점 및 코드 제거 |
| **게임** | `{"playtime":45, "metacritic":88, "stores": [...]}` | `"평균 플레이 시간은 45시간이며, 비평가 점수는 88점입니다. Steam에서 구매 가능합니다."` | 핵심 정보만 요약하여 전달 |

### 문제 6: Gemini 모델 역할 분담 실패로 인한 할당량 조기 소진

  * **현상**: 더 똑똑하지만 할당량이 적은 `gemini-2.5-flash` 모델이, 더 가볍고 할당량이 넉넉한 `gemini-2.5-flash-lite` 모델보다 불필요하게 많이 호출되어 무료 사용량이 조기에 소진됩니다.
  * **근본 원인**: 모든 대화의 최종 마무리를 무조건 고성능 `gemini-2.5-flash` 모델에게 맡기는 **'중앙집권적'** 구조 때문입니다. "안녕?" 같은 간단한 인사조차 `gemini-2.5-flash-lite`가 판단한 후, 다시 `gemini-2.5-flash`에게 "안녕이라고 대답해"라고 보고하는 비효율적인 프로세스입니다.
  * **해결 방안**: \*\*'권한 위임'\*\*을 통해 모델의 역할을 재정의합니다. **이 모델 구성은 2025년 8월 기준 최적의 전략이므로 임의로 변경해서는 안 됩니다.**
      * **`gemini-2.5-flash-lite` (신속 대응반)**: ① 도구(API) 사용 여부 판단, ② **도구가 필요 없는 간단한 대화의 경우 직접 답변 생성까지 완료.** (예: 인사, 간단한 감정 표현)
      * **`gemini-2.5-flash` (심층 분석반)**: ① **도구를 사용한 후, 그 결과를 바탕으로 복잡한 최종 답변을 종합하고 생성할 때만 사용.** (예: 날씨 정보와 페르소나를 결합한 답변)
  * **개선 흐름**:
    1.  `사용자` → `gemini-2.5-flash-lite` (판단 및 간단 답변 시도)
    2.  `gemini-2.5-flash-lite`가 API 호출이 필요 없다고 판단하면, **그 자리에서 직접 답변을 생성하고 프로세스를 종료**합니다.
    3.  API 호출이 필요한 복잡한 경우에만, `gemini-2.5-flash-lite`가 API를 호출하고 그 결과를 `gemini-2.5-flash`에게 넘겨 최종 답변을 생성하도록 합니다.
  * **효과**: 간단한 대화는 모두 `gemini-2.5-flash-lite` 선에서 처리되므로, `gemini-2.5-flash`의 호출 횟수가 극적으로 감소하여 할당량을 보존하고 운영 비용을 절감합니다.

-----

## Phase 3: 지능 - 기능 심화 및 잠재력 극대화

### 문제 8: API 잠재력의 제한적 활용

  * **현상**: 날씨, 환율, 게임 API가 제공하는 풍부한 데이터 중 극히 일부만 사용하고 있어, 봇이 '수박 겉핥기' 식의 정보만 제공합니다.
  * **근본 원인**: 초기 개발 단계에서 가장 핵심적인 데이터에만 집중했기 때문입니다. 이제는 각 API의 문서를 깊이 파고들어, 무료로 제공되는 모든 '숨겨진 보석' 같은 데이터들을 발굴할 시간입니다.
  * **해결 방안**: \*\*'API 데이터 마이닝 및 컨텍스트 강화'\*\*를 수행합니다.

| **API** | **발굴할 추가 데이터** | **사용자 경험 개선** |
| :--- | :--- | :--- |
| **기상청** | - **습도(REH)**, **풍향/풍속(VEC/WSD)**\<br\>- **1시간 강수량(RN1)** | "기온은 25도지만, 습도가 85%라 후덥지근하고 초속 3m의 바람이 붑니다." 와 같이 **체감 날씨**에 가까운 생생한 정보를 제공합니다. |
| **수출입은행** | - **송금 보낼때/받을때(TTS/TTB)**\<br\>- **현찰 살때/팔때(TCB/TCB)** | "해외 직구 시에는 1달러당 1,350원이 적용되고, 여행 후 남은 달러를 바꿀 때는 1,300원을 받게 돼요." 와 같이 **사용자의 실제 상황에 맞는** 맞춤형 환율 정보를 제공합니다. |


### 신규 제안: 능동적 비서로의 진화

  * **대화 기억력 강화 (Context Management)**: 단기 기억(최근 몇 개의 메시지)을 넘어, 대화의 핵심 내용을 요약하여 저장하는 **'장기 기억'** 시스템을 도입합니다. 이를 통해 "아까 말했던 그 게임"과 같은 모호한 질문에도 맥락을 파악하여 답변할 수 있게 됩니다.
  * **사용자 맞춤형 알림 기능 (Proactive Notification)**: 사용자가 "달러 환율 1300원 아래로 내려가면 알려줘"라고 요청하면, 이 목표를 데이터베이스에 저장합니다. 이후 봇이 주기적으로 환율을 체크하여 조건이 충족되었을 때 사용자에게 먼저 말을 거는 **'능동적 비서'** 기능을 구현합니다.
  * **숨겨진 의도 파악 (Implicit Intent Recognition)**: 사용자의 직접적인 질문이 아니더라도, 대화의 맥락에서 잠재적인 요구를 파악하여 먼저 제안하는 기능을 추가합니다.
      * **예시**: 사용자가 "다음 달에 일본 여행 가려고" 라고 말하면, 봇이 "오, 좋네요\! 엔화 환율 정보를 알려드릴까요?" 라고 먼저 제안합니다.
      * **구현**: `gemini-2.5-flash-lite`에 전달하는 프롬프트에 "사용자의 발언에서 잠재적인 정보 요구를 파악하고, 만약 있다면 먼저 제안하는 역할을 수행해줘" 라는 지침을 추가하여 추론 능력을 활용합니다.

-----

## Phase 4: 생태계 - 확장성과 유지보수성

### 문제 9: 코드의 경직성과 낮은 유지보수성

  * **현상**: 새로운 기능을 추가하거나 기존 로직을 변경할 때 여러 파일을 수정해야 하고, 설정값이 코드 곳곳에 흩어져 있어 관리가 어렵습니다.
  * **근본 원인**: 초기 개발 단계에서는 빠른 구현에 집중하여 코드의 장기적인 구조와 확장성을 충분히 고려하지 못했습니다.
  * **해결 방안**:
    1.  **설정 중앙화 (Centralized Configuration)**: API URL, 기본 매개변수 등 코드에 하드코딩된 값들을 `config.json` 또는 별도의 설정 파일로 모두 이전합니다. 이를 통해 코드 수정 없이 설정 파일만 변경하여 봇의 동작을 제어할 수 있게 됩니다.
    2.  **구조적 로깅 (Structured Logging)**: 단순 텍스트 로그 대신, 타임스탬프, 로그 레벨, 사용자 ID, 서버 ID 등의 정보를 포함하는 JSON 형식의 로그를 사용합니다. 이는 나중에 로그 분석 시스템(예: ELK 스택)을 도입할 때 데이터 파싱을 용이하게 하여 문제 추적을 훨씬 쉽게 만듭니다.
    3.  **단위 테스트 도입 (Unit Testing)**: 각 기능(특히 API 핸들러)이 독립적으로 올바르게 작동하는지 검증하는 테스트 코드를 작성합니다. `pytest`와 같은 프레임워크를 사용하여, "특정 형태의 API 응답이 왔을 때, 데이터 정제 함수가 정확히 예상된 요약문을 반환하는가?" 와 같은 시나리오를 자동 검증합니다. 이는 향후 코드 변경 시 발생할 수 있는 예기치 않은 버그를 사전에 방지하는 안전망 역할을 합니다.

## 5\. 종합 결론

이 계획서는 마사몽 봇을 단순한 채팅 봇에서 **안정적이고, 효율적이며, 지능적인 정보 허브**로 탈바꿈시키기 위한 청사진입니다. 제안된 개선안들을 단계적으로 적용한다면, 봇은 사용자의 신뢰를 얻고, 운영 비용을 절감하며, 궁극적으로는 없어서는 안 될 강력한 어시스턴트로 자리매김할 것입니다.
